<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="brief-report"><?DTDIdentifier.IdentifierValue -//APA//DTD APA Journal Archive DTD v1.0 20130715//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName APAjournal-archive.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName apaja2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Emotion</journal-id><journal-id journal-id-type="iso-abbrev">Emotion</journal-id><journal-title-group><journal-title>Emotion (Washington, D.C.)</journal-title></journal-title-group><issn pub-type="ppub">1528-3542</issn><issn pub-type="epub">1931-1516</issn><publisher><publisher-name>American Psychological Association</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">emo_14_6_1007</article-id><article-id pub-id-type="doi">10.1037/a0037945</article-id><article-id pub-id-type="publisher-id">2014-41986-001</article-id><article-categories><subj-group subj-group-type="heading"><subject>Brief Report</subject></subj-group></article-categories><title-group><article-title>Facial Expression Influences Face Identity Recognition During the Attentional Blink</article-title><alt-title alt-title-type="article-banner">BRIEF REPORT</alt-title></title-group><contrib-group><contrib contrib-type="editor" corresp="no"><name><surname>DeSteno</surname><given-names>David</given-names></name><role>Editor</role></contrib></contrib-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Bach</surname><given-names>Dominik R.</given-names></name><xref rid="aff1" ref-type="aff">1</xref><xref rid="corr1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="no"><name><surname>Schmidt-Daffy</surname><given-names>Martin</given-names></name><xref rid="aff2" ref-type="aff">2</xref></contrib><contrib contrib-type="author" corresp="no"><name><surname>Dolan</surname><given-names>Raymond J.</given-names></name><xref rid="aff3" ref-type="aff">3</xref></contrib><aff id="aff1"><label>1</label>Wellcome Trust Centre for Neuroimaging, University College London, and Department of Psychiatry, Psychotherapy, and Psychosomatics, University of Zurich</aff><aff id="aff2"><label>2</label>Department of Psychology and Ergonomics, Berlin Institute of Technology</aff><aff id="aff3"><label>3</label>Wellcome Trust Centre for Neuroimaging, University College London</aff></contrib-group><author-notes><p>This work was supported by the Wellcome Trust (Raymond J. Dolan, Senior Investigator Award 098362/Z/12/Z) and the Swiss National Science Foundation (Dominik R. Bach, Advanced Postdoc Mobility Fellowship PA117384). The Wellcome Trust Centre for Neuroimaging is supported by core funding from the Wellcome Trust 091593/Z/10/Z. We thank Daniela Graf, Gisela Erdmann, and Erich Seifritz, for supporting Experiment 1. We thank Arezoo Pooresmaeili for stimulating discussion; Christoph Korn provided valuable comments on a first version of this article.</p><corresp id="corr1"><label>*</label>Correspondence concerning this article should be addressed to Dominik R. Bach, Department of Psychiatry, Psychotherapy, and Psychosomatics, University of Zurich, Lenggstrasse 31, 8032 Zurich, Switzerland <email>dominik.bach@uzh.ch</email></corresp></author-notes><pub-date pub-type="epub"><day>6</day><month>10</month><year>2014</year></pub-date><pub-date pub-type="ppub"><month>12</month><year>2014</year></pub-date><volume>14</volume><issue>6</issue><fpage>1007</fpage><lpage>1013</lpage><history><date date-type="received"><day>28</day><month>11</month><year>2013</year></date><date date-type="rev-recd"><day>7</day><month>7</month><year>2014</year></date><date date-type="accepted"><day>22</day><month>7</month><year>2014</year></date></history><permissions><copyright-statement>&#x000a9; 2014 The Author(s)</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article has been published under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/" specific-use="live">http://creativecommons.org/licenses/by/3.0/</ext-link>), which permits unrestricted use, distribution and reproduction in any medium, provided the original author and source are credited. Copyright of this article is retained by the author(s). Author(s) grant(s) the American Pyschological Association the exclusive right to publish the article and identify itself as the original publisher.</license-p></license></permissions><abstract><p>Emotional stimuli (e.g., negative facial expressions) enjoy prioritized memory access when task relevant, consistent with their ability to capture attention. Whether emotional expression also impacts on memory access when task-irrelevant is important for arbitrating between feature-based and object-based attentional capture. Here, the authors address this question in 3 experiments using an attentional blink task with face photographs as first and second target (T1, T2). They demonstrate reduced neutral T2 identity recognition after angry or happy T1 expression, compared to neutral T1, and this supports attentional capture by a task-irrelevant feature. Crucially, after neutral T1, T2 identity recognition was enhanced and not suppressed when T2 was angry&#x02014;suggesting that attentional capture by this task-irrelevant feature may be object-based and not feature-based. As an unexpected finding, both angry and happy facial expressions suppress memory access for competing objects, but only angry facial expression enjoyed privileged memory access. This could imply that these 2 processes are relatively independent from one another.</p></abstract><kwd-group><kwd>facial expression of emotion</kwd><kwd>visual search</kwd><kwd>automatic processing</kwd><kwd>attentional capture</kwd><kwd>threat detection</kwd></kwd-group></article-meta></front><body><p>Emotional facial expression is an important social signal, and there is considerable evidence for prioritized processing, particularly when it is negatively valenced (<xref rid="c4" ref-type="bibr" id="cr4-1">Bar-Haim, Lamy, Pergamin, Bakermans-Kranenburg, &#x00026; van IJzendoorn, 2007</xref>). This has been shown as increased detection speed for angry faces in a face-in-the-crowd task (<xref rid="c13" ref-type="bibr" id="cr13-1">Horstmann &#x00026; Bauland, 2006</xref>; <xref rid="c28" ref-type="bibr" id="cr28-1">Schmidt-Daffy, 2011</xref>), preferential spatial attention for arousing facial expression in a dot probe task (<xref rid="c18" ref-type="bibr" id="cr18-1">Macleod &#x00026; Mathews, 1988</xref>; <xref rid="c19" ref-type="bibr" id="cr19-1">MacLeod, Mathews, &#x00026; Tata, 1986</xref>), and privileged memory access for angry and fearful expression when capacity is limited in the attentional blink task (<xref rid="c6" ref-type="bibr" id="cr6-1">de Jong, Koster, van Wees, &#x00026; Martens, 2009</xref>; <xref rid="c11" ref-type="bibr" id="cr11-1">Fox, Russo, &#x00026; Georgiou, 2005</xref>; <xref rid="c16" ref-type="bibr" id="cr16-1">Luo, Feng, He, Wang, &#x00026; Luo, 2010</xref>; <xref rid="c21" ref-type="bibr" id="cr21-1">Maratos, Mogg, &#x00026; Bradley, 2008</xref>; <xref rid="c24" ref-type="bibr" id="cr24-1">Milders, Sahraie, Logan, &#x00026; Donnellon, 2006</xref>). The latter comprises a rapid serial visual presentation (RSVP) stream with two embedded targets (T1, T2)&#x02014;for example, letters, words, or pictures. If T1 and T2 are separated by a few distractor items, presence or identity of the T2 is less well reported than if it follows the T1 either immediately, or after an interval longer than 500&#x02013;800 ms, a phenomenon termed <italic>attentional blink</italic> (AB) (<xref rid="c5" ref-type="bibr" id="cr5-1">Chun &#x00026; Potter, 1995</xref>; <xref rid="c14" ref-type="bibr" id="cr14-1">Isaak, Shapiro, &#x00026; Martin, 1999</xref>; <xref rid="c27" ref-type="bibr" id="cr27-1">Raymond, Shapiro, &#x00026; Arnell, 1992</xref>). During this AB period, arousing T2 words are less susceptible to the blink phenomenon than nonarousing ones (<xref rid="c1" ref-type="bibr" id="cr1-1">Anderson, 2005</xref>; <xref rid="c2" ref-type="bibr" id="cr2-1">Anderson &#x00026; Phelps, 2001</xref>; <xref rid="c9" ref-type="bibr" id="cr9-1">De Martino, Kalisch, Rees, &#x00026; Dolan, 2009</xref>; <xref rid="c15" ref-type="bibr" id="cr15-1">Keil &#x00026; Ihssen, 2004</xref>), in line with an account that the &#x0201c;blink&#x0201d; phenomenon occurs between target detection and memory encoding (<xref rid="c10" ref-type="bibr" id="cr10-1">Dux, Ivanoff, Asplund, &#x00026; Marois, 2006</xref>). It thus reflects impaired access to memory, but not impaired initial (possibly preconscious) detection. Similarly, the presence of angry and fearful T2 faces is better recalled, and their expression better remembered, than neutral T2 (<xref rid="c6" ref-type="bibr" id="cr6-2">de Jong et al., 2009</xref>; <xref rid="c11" ref-type="bibr" id="cr11-2">Fox et al., 2005</xref>; <xref rid="c16" ref-type="bibr" id="cr16-2">Luo et al., 2010</xref>; <xref rid="c21" ref-type="bibr" id="cr21-2">Maratos et al., 2008</xref>; <xref rid="c24" ref-type="bibr" id="cr24-2">Milders et al., 2006</xref>). Evidence is somewhat less clear for happy expression (<xref rid="c6" ref-type="bibr" id="cr6-3">de Jong et al., 2009</xref>; <xref rid="c11" ref-type="bibr" id="cr11-3">Fox et al., 2005</xref>; <xref rid="c17" ref-type="bibr" id="cr17-1">Mack, Pappas, Silverman, &#x00026; Gay, 2002</xref>; <xref rid="c25" ref-type="bibr" id="cr25-1">Miyazawa &#x00026; Iwasaki, 2010</xref>), and there is a suggestion of privileged memory access for angry as compared to happy facial expression in T2 position (<xref rid="c8" ref-type="bibr" id="cr8-1">de Jong &#x00026; Martens, 2007</xref>). In addition, angry faces in T1 position suppress T2 recognition (<xref rid="c7" ref-type="bibr" id="cr7-1">de Jong, Koster, van Wees, &#x00026; Martens, 2010</xref>; <xref rid="c20" ref-type="bibr" id="cr20-1">Maratos, 2011</xref>), suggesting prioritized processing of an angry face in T1 position as well.</p><p>These observations indicate that negative, and possibly also positive, facial expressions capture attention. It is striking that all aforementioned studies have tasked participants to remember the emotional expression of the targets. An important question is whether and how emotional facial expression impacts memory access when it is task-irrelevant and instead face identity is relevant for the task. It has been proposed that features that are close together in time or space form objects, and that attention is, as a rule, directed toward objects and not toward individual features (<xref rid="c22" ref-type="bibr" id="cr22-1">Mather, 2007</xref>). This possibility has been addressed for emotional words and nonfacial pictures where task-irrelevant arousing distractors presented close to a neutral target stimulus in a RSVP suppress memory for the target, a phenomenon termed <italic>emotional AB</italic> (<xref rid="c23" ref-type="bibr" id="cr23-1">McHugo, Olatunji, &#x00026; Zald, 2013</xref>; <xref rid="c26" ref-type="bibr" id="cr26-1">Most, Chun, Widders, &#x00026; Zald, 2005</xref>). Other experiments have demonstrated that when colored words with task-irrelevant semantic meaning are embedded in a RSPV and the task is to report the color, performance is better for emotional compared to neutral words (<xref rid="c3" ref-type="bibr" id="cr3-1">Arend, Botella, &#x00026; Barrada, 2003</xref>). What both types of experiments demonstrate is attentional capture by task-irrelevant features. Importantly, in one case, an arousing, task-irrelevant feature grabs attention and a task-relevant feature is suppressed while in the other case a concomitant task-relevant feature is prioritized. This discrepancy is resolved by recognizing that in the first case, the task-relevant feature belongs to a different object, and in the second case, it belongs to the same object. A framework of object-based attention predicts that if a salient feature captures attention, then the entire object enjoys privileged memory access, whereas features of other objects are likely to be suppressed (<xref rid="c22" ref-type="bibr" id="cr22-2">Mather, 2007</xref>). This framework predicts that attentional capture by emotional facial expression would also improve identity processing of the same face.</p><p>In the present study, we addressed attentional capture by task-irrelevant features for angry and happy facial expression. First, we were interested whether emotional expression in angry or happy face photographs can capture attention at all when it is task-irrelevant. In fact, for fearful faces there is mixed evidence (<xref rid="c24" ref-type="bibr" id="cr24-3">Milders et al., 2006</xref>; <xref rid="c30" ref-type="bibr" id="cr30-1">Stein, Zwickel, Ritter, Kitzmantel, &#x00026; Schneider, 2009</xref>), and it is suggested that when they are task-relevant, attentional capture is not automatic but can be reduced by increasing distraction (<xref rid="c29" ref-type="bibr" id="cr29-1">Stein, Peelen, Funk, &#x00026; Seidl, 2010</xref>). In a visual search task, attentional capture by irrelevant facial expression has been established for happy, but not angry or fearful expression (<xref rid="c12" ref-type="bibr" id="cr12-1">Hodsoll, Viding, &#x00026; Lavie, 2011</xref>). Hence, it remains an open question whether irrelevant angry or happy facial expression in T1 and T2 position can capture attention in the AB setup. Second, we were interested in whether the impact of attentional capture is based on objects or on features. In the attentional blink paradigm, T1 and T2 are two clearly separable objects; in our implementation they are face photographs with different identity. This predicts that if attentional capture is based on objects, emotional expression of one target should enhance reporting the identity of the same target, and suppress reporting identity of the other target. However, if attentional capture is based on features, then emotional expression of one target should suppress reporting identity of the same and of the other target.</p><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s3"><title>Subjects</title><p>Forty female university students (<italic>M</italic> age &#x000b1; <italic>SD</italic> = 26.9 &#x000b1; 3.5 years) volunteered for Experiment 1, 20 students (eight male, 12 female, 24.6 &#x000b1; 5.0 years) for Experiment 2, and 24 students (seven male, 17 female, 23.2 &#x000b1; 3.8 years) for Experiment 3; all samples were independent from one another. The study was approved by the local research ethics committee.</p></sec><sec id="s4"><title>Design and Independent Variables</title><p>Experiment 1 followed a 3 (T1 valence) &#x000d7; 3 (T2 valence) &#x000d7; 5 (T1-T2 lag) factorial design, and Experiments 2 and 3 followed a 3 (T1 [Exp. 3] or T2 [Exp. 2] valence) &#x000d7; 10 (T1-T2 lag) factorial design. In one experimental block, three male faces appeared as T1 and three female faces as T2, and vice versa in the second block, whereas the sequence of the two blocks was balanced across participants. For each cell of the design (trial type), each of three actors for T1 and T2, respectively, was presented twice per block. In Experiment 1 we were mainly interested in trials with neutral T1 (15 trial types, 180 trials). Therefore, each actor was only presented once per block in other trial types (30 trial types, 180 trials). This procedure resulted in overall 360 trials for each of the three experiments. Trials were intermixed randomly, and T1-T2 actor combinations in single trials were varied randomly across subjects. In Experiments 2 and 3, we added an additional 36 single-target trials, presenting each of the six actors with three facial expressions twice. We report trials with the same expression as the T2 targets for comparison with the two-target trials.</p></sec><sec id="s5"><title>Dependent Variables</title><p>After each trial, participants were tasked to recognize the identity of the faces previously shown as T1 and T2, respectively, by choosing one out of three simultaneously presented faces (all with the same sex and emotional expression as T1 and T2, respectively). Dependent variable for all analyses was T2 recognition, averaged across all trials where T1 was recognized correctly.</p></sec><sec id="s6"><title>Apparatus</title><p>Experiment 1 was programmed in e-prime (Psychology Software Tools, Sharpsburg, PA) and presented on a 14-in LCD screen. Experiments 2 and 3 were programmed in Cogent 2000 (<ext-link ext-link-type="uri" xlink:href="http://www.vislab.ucl.ac.uk" specific-use="live">www.vislab.ucl.ac.uk</ext-link>) and presented on a 20-in LCD screen, using a chin rest for constant viewing distance. Screen refresh rate was 60 Hz and screen resolution 1024 &#x000d7; 768 pixels for all experiments.</p></sec><sec id="s7"><title>Stimuli</title><p>Face stimuli (see example in <xref ref-type="fig" id="fgc1-1" rid="fig1">Figure 1</xref>) were modified pictures of facial affect (<xref rid="c31" ref-type="bibr" id="cr31-1">Ekman &#x00026; Friesen, 1976</xref>). Details about stimulus construction are given in (<xref rid="c28" ref-type="bibr" id="cr28-2">Schmidt-Daffy, 2011</xref>). Distractor stimuli were constructed by cutting pictures of facial affect into squares of 5 &#x000d7; 10 pixels, randomly combining them and applying the background mask of target stimuli (see <xref ref-type="fig" id="fgc1-2" rid="fig1">Figure 1</xref>). All stimuli were presented on black background with a size of 74 &#x000d7; 101 pixel (visual angle of 1.72 &#x000d7;2.29).<xref ref-type="fig-anchor" rid="fig1"/></p></sec><sec id="s8"><title>Procedure</title><p>Each experiment consisted of 25 practice trials, followed by two blocks of 180 trials each. After a fixation cross (1,000 ms), face stimuli were shown at 10 Hz (4 frames/&#x0223c;67 ms stimulus, 2 frames/&#x0223c;33 ms blank). Five, 10, or 15 distractors were presented before T1. T1 and T2 were separated by 0, 1, 2, 3, or 7 distractors (Experiment 1) or 0&#x02013;9 distractors (Experiments 2 and 3). After T2, five more distractors were shown. For single-target trials in Experiments 2 and 3, a single target was shown after five, 10, or 15 distractors and was followed by five distractors. After the trial, the three possible T1 identities and the three possible T2 identities were presented on two subsequent screens, from which participants selected with a key press.</p></sec><sec id="s9"><title>Data Analysis</title><p>For each subject, T2 response accuracy was extracted and averaged across stimuli for all trials on which T1 was correctly identified. Statistical analyses were performed with SPSS, using a 3 &#x000d7; 5 (Experiment 1), 3 &#x000d7; 3 &#x000d7; 5 (Full model, Experiment 1) or 3 &#x000d7; 10 analysis of variance. Results are stated in <xref ref-type="table" id="tbc1-1" rid="tbl1">Table 1</xref>, and post hoc tests as well are stated in the main text. In an exploratory approach, T1 recognition was tested in a univariate 3 &#x000d7; 10 analysis of variance. Results are stated in the main text.<xref ref-type="table-anchor" rid="tbl1"/></p></sec></sec><sec sec-type="results" id="s10"><title>Results</title><sec id="s11"><title>Experiment 1</title><sec id="s12"><title>Recognition of valence-varied T2 after neutral T1</title><p>T1-T2 lag impacted the recognition of valence-varied T2 after correctly recognized neutral T1, as standardly observed in AB tasks, with a <sans-serif>U</sans-serif>-shaped time course (<xref ref-type="table" id="tbc1-2" rid="tbl1">Table 1</xref>, <xref ref-type="fig" id="fgc2-1" rid="fig2">Figure 2</xref>). As hypothesized, T2 valence had a significant impact on T2 recognition. There was no interaction with lag. Post hoc contrasts revealed that happy face identity was recognized less well than neutral, <italic>F</italic>(1, 39) = 11.3, <italic>p</italic> &#x0003c; .01, or angry, <italic>F</italic>(1, 39) = 9.1, <italic>p</italic> &#x0003c; .01, face identity. T2 recognition at Lag 8 did not recover to the level of Lag 1 across all T2 (contrast Lag 8 vs. Lag 1: all T2, <italic>F</italic>(1, 9) = 7.3, <italic>p</italic> &#x0003c; .01), implying an AB period that is longer than reported previously.<xref ref-type="fig-anchor" rid="fig2"/></p></sec><sec id="s13"><title>Recognition of neutral T2 after valence-varied T1</title><p>T1-T2 lag had a highly significant impact on the recognition of neutral T2 after correctly recognized valence-varied T1 (see <xref ref-type="table" id="tbc1-3" rid="tbl1">Table 1</xref>). There was no difference between Lag 1 and Lag 8 recognition across all T1 or for any of the T1 valence categories. Post hoc contrasts revealed that angry T1 expression impaired T2 recognition compared to neutral, <italic>F</italic>(1, 39) = 13.0, <italic>p</italic> &#x0003c; .001, and happy, <italic>F</italic>(1, 39) = 8.1, <italic>p</italic> &#x0003c; .01, expression. There was no interaction with lag.</p></sec><sec id="s14"><title>Full model</title><p>A full model, including combinations of valence-varied T1 and valence-varied T2 confirmed the impact of lag (see <xref ref-type="table" id="tbc1-4" rid="tbl1">Table 1</xref>). Across all combinations, there was no performance difference between Lags 1 and 8. We observed a significant impact of T1 but not of T2 valence on T2 recognition and a trend-level interaction of T1 valence and lag. T2 recognition was impaired after angry T1 for all lags, and after happy T1 for medium lags alone.</p><p>Taken together, this experiment confirms the known time course of the AB for the identification of valence-varied faces, with the exception that recognition at Lag 8 did not fully recover after a neutral T1. We observed an impact both of T1 and T2 valence on T2 recognition when analyzing combinations in which at least one target was neutral. In a full model, the effect of T1 was clearly dominant. We did not observe a significant interaction of target valence and lag, indicating that the valence effects might not arise from capacity limits during the AB period. However, because finding an interaction relies on using lags after the AB period, and because the AB period in this experiment appeared to be longer than Lag 8 for some target combinations, this observation might also be due to a lack of power. Hence, Experiments 2 and 3 were conducted to more fully sample lags from within and outside the AB period and to investigate valence-varied target recognition in the absence of interfering items, that is, in single-target trials. Also, we separated manipulation of T1 and T2 valence into two separate experiments to increase the number of trials per condition, reduce noise in the averages, and thus enhance statistical power.</p></sec></sec><sec id="s15"><title>Experiment 2</title><p>This experiment was designed to separately assess the impact of T2 facial expression on T2 identity recognition after neutral T1 without interference from other trial types, using 10 lags, and 12 trials per cell of the design, thereby doubling the number of trials per cell, and hence, sensitivity. We hypothesized a replication of the T2 valence effect observed in Experiment 1.</p><sec id="s16"><title>Valence-varied T2 recognition after neutral T1</title><p>Lag impacted recognition of valence-varied T2 after neutral T1 (see <xref ref-type="table" id="tbc1-5" rid="tbl1">Table 1</xref>). Recognition at Lags 1 and 10 was not different for any T2 valence, or across all T2 valences, indicating that we fully sampled the entire AB period. T2 valence significantly influenced T2 recognition, which was better for angry than for happy, <italic>F</italic>(1, 19) = 20.8, <italic>p</italic> &#x0003c; .001, or neutral T2, <italic>F</italic>(1, 19) = 9.5, <italic>p</italic> &#x0003c; .01. There was no interaction of T2 valence and lag. A similar, trend-level significant impact of target valence was found on single target recognition, <italic>F</italic>(2, 38) = 3.0, <italic>p</italic> = .077. Hence, we corrected T2 recognition in AB trials for the single-trial recognition, after which the T2 valence effect vanished, <italic>F</italic>(2, 38) &#x0003c; 1, n.s.). Therefore, the impact of T2 valence in this experiment was not dependent on the specific capacity limits during the AB.</p></sec><sec id="s17"><title>T1 recognition</title><p>The AB paradigm is designed to measure impairment in T2 recognition with generally much higher global recognition rates for T1 than for T2. Nevertheless, we analyzed T1 recognition in an exploratory approach. Both for global T1 recognition and for T1 recognition before correctly recognized T2, T1 recognition depended on lag, global: <italic>F</italic>(9, 171) = 14.5, <italic>p</italic> &#x0003c; .001; before correct T2:, <italic>F</italic>(9, 171) = 11.7, <italic>p</italic> &#x0003c; .001, but not on T2 expression, global: <italic>F</italic>(2, 38) &#x0003c; 1, n.s.; before correct T2: <italic>F</italic>(2, 38) &#x0003c; 1, n.s.</p></sec></sec><sec id="s18"><title>Experiment 3</title><p>This experiment was designed to separately assess the impact of T1 facial expression on neutral T2 identity recognition without interference from other trial types, using 10 lags, and 12 trials per cell of the design. We hypothesized a replication of the T1 valence effect on T2 identity recognition observed in Experiment 1.</p><sec id="s19"><title>Neutral T2 recognition after valence-varied T1</title><p>Lag impacted neutral T2 recognition after valence-varied T1 (see <xref ref-type="table" id="tbc1-6" rid="tbl1">Table 1</xref>). Performance at Lag 10 was slightly better than at Lag 1, <italic>F</italic>(1, 23) = 3.9, <italic>p</italic> = .06. There was no main effect of T1 valence on T2 recognition, but a significant interaction of T1 valence and lag: According to post hoc contrasts, T2 recognition was worse after angry/happy T1 than after neutral T1, for Lags 2&#x02013;6 compared to all other lags, <italic>F</italic>(1, 23) = 34.1, <italic>p</italic> &#x0003c; .001. There was no evidence for a differential effect of angry and happy T1 on T2 recognition.</p></sec><sec id="s20"><title>T1 recognition</title><p>T1 recognition was analyzed in an exploratory approach. Both for global analysis of T1 and for T1 before correctly recognized T2, lag had a significant effect on T1 recognition&#x02014;global: <italic>F</italic>(9, 207) = 30.9, <italic>p</italic> &#x0003c; .001; before correct T2:, <italic>F</italic>(9, 207) = 43.9, <italic>p</italic> &#x0003c; .001. Also, valence had a significant effect&#x02014;global: <italic>F</italic>(2, 46) = 49.4, <italic>p</italic> &#x0003c; .001; before correct T2: <italic>F</italic>(2, 46) = 20.6, <italic>p</italic> &#x0003c; .001. Post hoc contrasts revealed that angry T1 were recognized better than neutral&#x02014;global: <italic>F</italic>(1, 23) = 49.4, <italic>p</italic> &#x0003c; .001; before correct T2: <italic>F</italic>(1, 23) = 38.9, <italic>p</italic> &#x0003c; .001&#x02014;or happy T1&#x02014;global: <italic>F</italic>(1, 23) = 49.4, <italic>p</italic> &#x0003c; .001; before correct T2: <italic>F</italic>(1, 23) = 27.6, <italic>p</italic> &#x0003c; .001.</p></sec></sec></sec><sec sec-type="discussion" id="s21"><title>Discussion</title><p>We investigated whether attentional capture by emotional facial expression occurs when task-irrelevant and whether it impacts processing of facial identity. Three main findings emerge. First, task-irrelevant emotional facial expression in T1 or T2 position influenced T2 identity recognition, suggesting it automatically captures attention. Emotional expression increased memory for face identity of the same target, but suppressed memory for face identity of other targets. This confirms predictions of an object-based framework of attentional capture by facial identity. Lastly, angry but not happy target faces enjoyed privileged identity recognition, independent from capacity limits, whereas in Experiment 3, both angry and happy expression suppressed identity recognition of the other target, dependent on capacity limits.</p><p>Our experiments confirm that attention to facial expression, and to face identity, are not separable. This is in keeping with the predictions of an object-based attention framework according to which objects are formed from basic features, and attention is directed toward objects, not toward individual features. On the other hand, a possibility that facial identity and face expression are composed of overlapping sets of basic physical features could, in theory, also explain the inseparability of facial expression and face identity. This possibility cannot be ruled out in the present study, although we note that there is no entirely convincing evidence for this in the literature.</p><p>The pattern of angry T2 advantage contrasts with the pattern of T2 suppression by arousing (angry and happy) T1 with specific AB influence in Experiment 3. Importantly, exploratory analysis of Experiment 3 demonstrated that although both angry and happy T1 equally suppressed T2 recognition, T1 recognition was only enhanced for angry but not for happy T1. The latter is the same pattern as angry T2 advantage in Experiments 1 and 2. In other words, happy faces in T1 position suppress T2 memory but do not enjoy privileged memory access themselves. This suggests that object-based attentional capture is not a unitary process in which advantage for the attention-grabbing object, and suppression of other objects, are two sides of the same coin. In fact, privileged memory access might be a process independent from suppressed memory access for other objects.</p><p>As a tentative model, we speculate that privileged memory access for angry faces depends on simple perceptual features that reliably differentiate possible threat from nonthreat. This would explain why it is relatively independent from capacity limits. In this model, privileged memory access is a direct consequence of stimulus attributes and not a consequence of processing limits. It is interesting that the anger detection advantage in the face-in-the-crowd task has already been explained with a sensory-bias hypothesis that states that particular perceptual features account for the impact of angry faces (<xref rid="c13" ref-type="bibr" id="cr13-2">Horstmann &#x00026; Bauland, 2006</xref>). On the other hand, the arousing quality of both angry and happy faces might bind resources for processing these objects and therefore suppresses processing access for other objects at a stage before any of these objects would enter memory, but only under limits in processing resources. Reduced processing of competing objects could explain reduced access to memory, yet without necessarily causing privileged access to memory for the arousing object. Reduced memory access for competing objects, in this model, is a consequence of processing limits at stages before objects are encoded into memory. An alternative, conceptually similar explanation is that arousing facial expression does not actually suppress memory encoding of competing faces but rather interferes with retrieval of competing faces. This is because T1 is always recalled before T2&#x02014;hence an arousing T1 facial expression could lead to increased processing upon retrieval and interfere with subsequent retrieval of a neutral T2. This might be supported by the observation that arousing T1 interferes with neutral T2 recognition, but arousing T2 does not interfere with neutral T1 recognition. Changing the order of retrieval might arbitrate between these two explanations.</p><p>This model critically relies on the effects of happy face stimuli in T1 position of our paradigm for which we observed an unexpected discrepancy between lack of privileged memory access and suppression of memory for other objects. Notably, although the impact of valence-varied T1 showed the same ordering in Experiments 1 and 3, the pattern described here was only observed in the better-powered Experiment 3, and this may imply a variability in this phenomenon. To corroborate the independence of privileged memory access, and suppressed memory access for other objects, it would be necessary to replicate these findings using other stimuli. Whether perceptual features of angry faces truly account for the observed privileged memory access is difficult to assess in the present paradigm. Previous work has used inverted or schematic faces to control for perceptual features, but this not only changes the emotional meaning of a face but also the perception of its identity. We hope in future work to address the question of perceptual features in a more elaborate fashion.</p><p>Previous studies have used the attentional blink paradigm to investigate attentional capture by fearful expression and have found that faces with task-irrelevant fearful expression enjoy privileged memory access in T2 position (<xref rid="c24" ref-type="bibr" id="cr24-4">Milders et al., 2006</xref>) but that fearful expression in T1 position suppresses T2 memory only when it is task-relevant (<xref rid="c30" ref-type="bibr" id="cr30-2">Stein et al., 2009</xref>). The latter findings clearly contradict automatic attentional capture. In this experiment, gender was the task-relevant dimension. Using angry, happy, and fearful face expressions in the same experiment, varying the same task-relevant dimension, and controlling for arousal of the face expression might clarify the discrepancy between this experiment and ours.</p><p>In summary, we observed attentional capture by task-irrelevant emotional expression that is broadly in keeping with an object-based attentional capture. As an unexpected finding, our data might suggest, if replicated, that privileged memory access for the attention-grabbing object, and memory suppression for competing objects, could be two independent processes.</p></sec></body><back><ref-list><title>References</title><ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>A. K.</given-names></name></person-group> (<year>2005</year>). <article-title>Affective influences on the attentional dynamics supporting awareness</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>134</volume>, <fpage>258</fpage>&#x02013;<lpage>281</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-3445.134.2.258</pub-id><pub-id pub-id-type="pmid">15869349</pub-id></mixed-citation></ref><ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>A. K.</given-names></name>, &#x00026; <name><surname>Phelps</surname><given-names>E. A.</given-names></name></person-group> (<year>2001</year>). <article-title>Lesions of the human amygdala impair enhanced perception of emotionally salient events</article-title>. <source>Nature</source>, <volume>411</volume>, <fpage>305</fpage>&#x02013;<lpage>309</lpage>. doi:<pub-id pub-id-type="doi">10.1038/35077083</pub-id><pub-id pub-id-type="pmid">11357132</pub-id></mixed-citation></ref><ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arend</surname><given-names>I.</given-names></name>, <name><surname>Botella</surname><given-names>J.</given-names></name>, &#x00026; <name><surname>Barrada</surname><given-names>J. R.</given-names></name></person-group> (<year>2003</year>). <article-title>Emotional load and the formation of illusory conjunctions in the time domain</article-title>. <source>Psicothema</source>, <volume>15</volume>, <fpage>446</fpage>&#x02013;<lpage>451</lpage>.</mixed-citation></ref><ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Haim</surname><given-names>Y.</given-names></name>, <name><surname>Lamy</surname><given-names>D.</given-names></name>, <name><surname>Pergamin</surname><given-names>L.</given-names></name>, <name><surname>Bakermans-Kranenburg</surname><given-names>M. J.</given-names></name>, &#x00026; <name><surname>van IJzendoorn</surname><given-names>M. H.</given-names></name></person-group> (<year>2007</year>). <article-title>Threat-related attentional bias in anxious and nonanxious individuals: A meta-analytic study</article-title>. <source>Psychological Bulletin</source>, <volume>133</volume>, <fpage>1</fpage>&#x02013;<lpage>24</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0033-2909.133.1.1</pub-id><pub-id pub-id-type="pmid">17201568</pub-id></mixed-citation></ref><ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chun</surname><given-names>M. M.</given-names></name>, &#x00026; <name><surname>Potter</surname><given-names>M. C.</given-names></name></person-group> (<year>1995</year>). <article-title>A two-stage model for multiple target detection in rapid serial visual presentation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>21</volume>, <fpage>109</fpage>&#x02013;<lpage>127</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.21.1.109</pub-id><pub-id pub-id-type="pmid">7707027</pub-id></mixed-citation></ref><ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Jong</surname><given-names>P. J.</given-names></name>, <name><surname>Koster</surname><given-names>E. H. W.</given-names></name>, <name><surname>van Wees</surname><given-names>R.</given-names></name>, &#x00026; <name><surname>Martens</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Emotional facial expressions and the attentional blink: Attenuated blink for angry and happy faces irrespective of social anxiety</article-title>. <source>Cognition &#x00026; Emotion</source>, <volume>23</volume>, <fpage>1640</fpage>&#x02013;<lpage>1652</lpage>. doi:<pub-id pub-id-type="doi">10.1080/02699930802490227</pub-id>.</mixed-citation></ref><ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Jong</surname><given-names>P. J.</given-names></name>, <name><surname>Koster</surname><given-names>E. H.</given-names></name>, <name><surname>van Wees</surname><given-names>R.</given-names></name>, &#x00026; <name><surname>Martens</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Angry facial expressions hamper subsequent target identification</article-title>. <source>Emotion</source>, <volume>10</volume>, <fpage>727</fpage>&#x02013;<lpage>732</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0019353</pub-id><pub-id pub-id-type="pmid">21038957</pub-id></mixed-citation></ref><ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Jong</surname><given-names>P. J.</given-names></name>, &#x00026; <name><surname>Martens</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>Detection of emotional expressions in rapidly changing facial displays in high- and low-socially anxious women</article-title>. <source>Behaviour Research and Therapy</source>, <volume>45</volume>, <fpage>1285</fpage>&#x02013;<lpage>1294</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.brat.2006.10.003</pub-id><pub-id pub-id-type="pmid">17113566</pub-id></mixed-citation></ref><ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname><given-names>B.</given-names></name>, <name><surname>Kalisch</surname><given-names>R.</given-names></name>, <name><surname>Rees</surname><given-names>G.</given-names></name>, &#x00026; <name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2009</year>). <article-title>Enhanced processing of threat stimuli under limited attentional resources</article-title>. <source>Cerebral Cortex</source>, <volume>19</volume>, <fpage>127</fpage>&#x02013;<lpage>133</lpage>.<pub-id pub-id-type="pmid">18448453</pub-id></mixed-citation></ref><ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dux</surname><given-names>P. E.</given-names></name>, <name><surname>Ivanoff</surname><given-names>J.</given-names></name>, <name><surname>Asplund</surname><given-names>C. L.</given-names></name>, &#x00026; <name><surname>Marois</surname><given-names>R.</given-names></name></person-group> (<year>2006</year>). <article-title>Isolation of a central bottleneck of information processing with time-resolved FMRI</article-title>. <source>Neuron</source>, <volume>52</volume>, <fpage>1109</fpage>&#x02013;<lpage>1120</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2006.11.009</pub-id><pub-id pub-id-type="pmid">17178412</pub-id></mixed-citation></ref><ref id="c31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name>, &#x00026; <name><surname>Friesen</surname><given-names>W.</given-names></name></person-group> (<year>1976</year>). <source>Pictures of facial affect</source>. <publisher-loc>Palo Alto, CA</publisher-loc>: <publisher-name>Consulting Psychologists Press</publisher-name>.</mixed-citation></ref><ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>E.</given-names></name>, <name><surname>Russo</surname><given-names>R.</given-names></name>, &#x00026; <name><surname>Georgiou</surname><given-names>G. A.</given-names></name></person-group> (<year>2005</year>). <article-title>Anxiety modulates the degree of attentive resources required to process emotional faces</article-title>. <source>Cognitive, Affective &#x00026; Behavioral Neuroscience</source>, <volume>5</volume>, <fpage>396</fpage>&#x02013;<lpage>404</lpage>. doi:<pub-id pub-id-type="doi">10.3758/CABN.5.4.396</pub-id></mixed-citation></ref><ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodsoll</surname><given-names>S.</given-names></name>, <name><surname>Viding</surname><given-names>E.</given-names></name>, &#x00026; <name><surname>Lavie</surname><given-names>N.</given-names></name></person-group> (<year>2011</year>). <article-title>Attentional capture by irrelevant emotional distractor faces</article-title>. <source>Emotion</source>, <volume>11</volume>, <fpage>346</fpage>&#x02013;<lpage>353</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0022771</pub-id><pub-id pub-id-type="pmid">21500903</pub-id></mixed-citation></ref><ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horstmann</surname><given-names>G.</given-names></name>, &#x00026; <name><surname>Bauland</surname><given-names>A.</given-names></name></person-group> (<year>2006</year>). <article-title>Search asymmetries with real faces: Testing the anger-superiority effect</article-title>. <source>Emotion</source>, <volume>6</volume>, <fpage>193</fpage>&#x02013;<lpage>207</lpage>. doi:<pub-id pub-id-type="doi">10.1037/1528-3542.6.2.193</pub-id><pub-id pub-id-type="pmid">16768552</pub-id></mixed-citation></ref><ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isaak</surname><given-names>M. I.</given-names></name>, <name><surname>Shapiro</surname><given-names>K. L.</given-names></name>, &#x00026; <name><surname>Martin</surname><given-names>J.</given-names></name></person-group> (<year>1999</year>). <article-title>The attentional blink reflects retrieval competition among multiple rapid serial visual presentation items: Tests of an interference model</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>25</volume>, <fpage>1774</fpage>&#x02013;<lpage>1792</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0096-1523.25.6.1774</pub-id><pub-id pub-id-type="pmid">10641317</pub-id></mixed-citation></ref><ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keil</surname><given-names>A.</given-names></name>, &#x00026; <name><surname>Ihssen</surname><given-names>N.</given-names></name></person-group> (<year>2004</year>). <article-title>Identification facilitation for emotionally arousing verbs during the attentional blink</article-title>. <source>Emotion</source>, <volume>4</volume>, <fpage>23</fpage>&#x02013;<lpage>35</lpage>. doi:<pub-id pub-id-type="doi">10.1037/1528-3542.4.1.23</pub-id><pub-id pub-id-type="pmid">15053724</pub-id></mixed-citation></ref><ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>W.</given-names></name>, <name><surname>Feng</surname><given-names>W.</given-names></name>, <name><surname>He</surname><given-names>W.</given-names></name>, <name><surname>Wang</surname><given-names>N. Y.</given-names></name>, &#x00026; <name><surname>Luo</surname><given-names>Y. J.</given-names></name></person-group> (<year>2010</year>). <article-title>Three stages of facial expression processing: ERP study with rapid serial visual presentation</article-title>. <source>NeuroImage</source>, <volume>49</volume>, <fpage>1857</fpage>&#x02013;<lpage>1867</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.09.018</pub-id><pub-id pub-id-type="pmid">19770052</pub-id></mixed-citation></ref><ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>A.</given-names></name>, <name><surname>Pappas</surname><given-names>Z.</given-names></name>, <name><surname>Silverman</surname><given-names>M.</given-names></name>, &#x00026; <name><surname>Gay</surname><given-names>R.</given-names></name></person-group> (<year>2002</year>). <article-title>What we see: Inattention and the capture of attention by meaning</article-title>. <source>Consciousness and Cognition: An International Journal</source>, <volume>11</volume>, <fpage>488</fpage>&#x02013;<lpage>506</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S1053-8100(02)00028-4</pub-id></mixed-citation></ref><ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>C.</given-names></name>, &#x00026; <name><surname>Mathews</surname><given-names>A.</given-names></name></person-group> (<year>1988</year>). <article-title>Anxiety and the allocation of attention to threat</article-title>. <source>Quarterly Journal of Experimental Psychology: A, Human Experimental Psychology</source>, <volume>40</volume>, <fpage>653</fpage>&#x02013;<lpage>670</lpage>. doi:<pub-id pub-id-type="doi">10.1080/14640748808402292</pub-id></mixed-citation></ref><ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>C.</given-names></name>, <name><surname>Mathews</surname><given-names>A.</given-names></name>, &#x00026; <name><surname>Tata</surname><given-names>P.</given-names></name></person-group> (<year>1986</year>). <article-title>Attentional bias in emotional disorders</article-title>. <source>Journal of Abnormal Psychology</source>, <volume>95</volume>, <fpage>15</fpage>&#x02013;<lpage>20</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0021-843X.95.1.15</pub-id><pub-id pub-id-type="pmid">3700842</pub-id></mixed-citation></ref><ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maratos</surname><given-names>F. A.</given-names></name></person-group> (<year>2011</year>). <article-title>Temporal processing of emotional stimuli: The capture and release of attention by angry faces</article-title>. <source>Emotion</source>, <volume>11</volume>, <fpage>1242</fpage>&#x02013;<lpage>1247</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0024279</pub-id><pub-id pub-id-type="pmid">21942702</pub-id></mixed-citation></ref><ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maratos</surname><given-names>F. A.</given-names></name>, <name><surname>Mogg</surname><given-names>K.</given-names></name>, &#x00026; <name><surname>Bradley</surname><given-names>B. P.</given-names></name></person-group> (<year>2008</year>). <article-title>Identification of angry faces in the attentional blink</article-title>. <source>Cognition &#x00026; Emotion</source>, <volume>22</volume>, <fpage>1340</fpage>&#x02013;<lpage>1352</lpage>. doi:<pub-id pub-id-type="doi">10.1080/02699930701774218</pub-id><pub-id pub-id-type="pmid">19360116</pub-id></mixed-citation></ref><ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>Emotional arousal and memory binding: An object-based framework</article-title>. <source>Perspectives on Psychological Science</source>, <volume>2</volume>, <fpage>33</fpage>&#x02013;<lpage>52</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1745-6916.2007.00028.x</pub-id></mixed-citation></ref><ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McHugo</surname><given-names>M.</given-names></name>, <name><surname>Olatunji</surname><given-names>B. O.</given-names></name>, &#x00026; <name><surname>Zald</surname><given-names>D. H.</given-names></name></person-group> (<year>2013</year>). <article-title>The emotional attentional blink: What we know so far</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>7</volume>, <fpage>151</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fnhum.2013.00151</pub-id><pub-id pub-id-type="pmid">23630482</pub-id></mixed-citation></ref><ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milders</surname><given-names>M.</given-names></name>, <name><surname>Sahraie</surname><given-names>A.</given-names></name>, <name><surname>Logan</surname><given-names>S.</given-names></name>, &#x00026; <name><surname>Donnellon</surname><given-names>N.</given-names></name></person-group> (<year>2006</year>). <article-title>Awareness of faces is modulated by their emotional meaning</article-title>. <source>Emotion</source>, <volume>6</volume>, <fpage>10</fpage>&#x02013;<lpage>17</lpage>. doi:<pub-id pub-id-type="doi">10.1037/1528-3542.6.1.10</pub-id><pub-id pub-id-type="pmid">16637746</pub-id></mixed-citation></ref><ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miyazawa</surname><given-names>S.</given-names></name>, &#x00026; <name><surname>Iwasaki</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Do happy faces capture attention? The happiness superiority effect in attentional blink</article-title>. <source>Emotion</source>, <volume>10</volume>, <fpage>712</fpage>&#x02013;<lpage>716</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0019348</pub-id><pub-id pub-id-type="pmid">21038954</pub-id></mixed-citation></ref><ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Most</surname><given-names>S. B.</given-names></name>, <name><surname>Chun</surname><given-names>M. M.</given-names></name>, <name><surname>Widders</surname><given-names>D. M.</given-names></name>, &#x00026; <name><surname>Zald</surname><given-names>D. H.</given-names></name></person-group> (<year>2005</year>). <article-title>Attentional rubbernecking: Cognitive control and personality in emotion-induced blindness</article-title>. <source>Psychonomic Bulletin &#x00026; Review</source>, <volume>12</volume>, <fpage>654</fpage>&#x02013;<lpage>661</lpage>. doi:<pub-id pub-id-type="doi">10.3758/BF03196754</pub-id><pub-id pub-id-type="pmid">16447378</pub-id></mixed-citation></ref><ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raymond</surname><given-names>J. E.</given-names></name>, <name><surname>Shapiro</surname><given-names>L.</given-names></name>, &#x00026; <name><surname>Arnell</surname><given-names>K. M.</given-names></name></person-group> (<year>1992</year>). <article-title>Temporary suppression of visual processing in an RSVP task: An attentional blink?</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>18</volume>, <fpage>849</fpage>&#x02013;<lpage>860</lpage>.<pub-id pub-id-type="pmid">1500880</pub-id></mixed-citation></ref><ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt-Daffy</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Modeling automatic threat detection: Development of a face-in-the-crowd task</article-title>. <source>Emotion</source>, <volume>11</volume>, <fpage>153</fpage>&#x02013;<lpage>168</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0022018</pub-id><pub-id pub-id-type="pmid">21401235</pub-id></mixed-citation></ref><ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>T.</given-names></name>, <name><surname>Peelen</surname><given-names>M. V.</given-names></name>, <name><surname>Funk</surname><given-names>J.</given-names></name>, &#x00026; <name><surname>Seidl</surname><given-names>K. N.</given-names></name></person-group> (<year>2010</year>). <article-title>The Fearful-face advantage is modulated by task demands: Evidence from the attentional blink</article-title>. <source>Emotion</source>, <volume>10</volume>, <fpage>136</fpage>&#x02013;<lpage>140</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0017814</pub-id><pub-id pub-id-type="pmid">20141310</pub-id></mixed-citation></ref><ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>T.</given-names></name>, <name><surname>Zwickel</surname><given-names>J.</given-names></name>, <name><surname>Ritter</surname><given-names>J.</given-names></name>, <name><surname>Kitzmantel</surname><given-names>M.</given-names></name>, &#x00026; <name><surname>Schneider</surname><given-names>W. X.</given-names></name></person-group> (<year>2009</year>). <article-title>The effect of fearful faces on the attentional blink is task dependent</article-title>. <source>Psychonomic Bulletin &#x00026; Review</source>, <volume>16</volume>, <fpage>104</fpage>&#x02013;<lpage>109</lpage>. doi:<pub-id pub-id-type="doi">10.3758/PBR.16.1.104</pub-id><pub-id pub-id-type="pmid">19145018</pub-id></mixed-citation></ref></ref-list></back><floats-group><table-wrap id="tbl1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Lag &#x000d7; Valence Repeated-Measures Analysis of Variance for the Three Experiments</title></caption><alternatives><graphic id="tbl1a" xlink:href="emo_14_6_1007_tbl1a"/><table frame="hsides" rules="groups"><colgroup span="1"><col span="1"/><col char="." span="1"/><col char="." span="1"/><col char="." span="1"/><col char="." span="1"/><col char="." span="1"/></colgroup><thead><tr valign="bottom"><th rowspan="1" colspan="1">Effect</th><th rowspan="1" colspan="1"><italic>df</italic></th><th rowspan="1" colspan="1">&#x003b5;</th><th rowspan="1" colspan="1">&#x003b7;<sup>2</sup></th><th rowspan="1" colspan="1"><italic>F</italic></th><th rowspan="1" colspan="1"><italic>p</italic></th></tr></thead><tfoot valign="top"><tr><td colspan="6" rowspan="1"><italic>Note</italic>.&#x02003;<italic>p</italic> values were computed after correcting degrees of freedom according to Greenhouse-Geisser. T1 = Target; T2 = Target 2.</td></tr></tfoot><tbody><tr valign="top"><td rowspan="1" colspan="1">Experiment 1: Valence-varied T2 after neutral T1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag</td><td rowspan="1" colspan="1">4,156</td><td rowspan="1" colspan="1">.995</td><td rowspan="1" colspan="1">.332</td><td rowspan="1" colspan="1">19.4</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T2 valence</td><td rowspan="1" colspan="1">2,78</td><td rowspan="1" colspan="1">.998</td><td rowspan="1" colspan="1">.138</td><td rowspan="1" colspan="1">6.3</td><td rowspan="1" colspan="1">&#x0003c;.01</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag &#x000d7; T2 valence</td><td rowspan="1" colspan="1">8,312</td><td rowspan="1" colspan="1">1.000</td><td rowspan="1" colspan="1">.010</td><td rowspan="1" colspan="1">&#x0003c;1</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">Experiment 1: Neutral T2 after valence-varied T1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag</td><td rowspan="1" colspan="1">4,148</td><td rowspan="1" colspan="1">.967</td><td rowspan="1" colspan="1">.278</td><td rowspan="1" colspan="1">14.3</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T1 valence</td><td rowspan="1" colspan="1">2,74</td><td rowspan="1" colspan="1">.868</td><td rowspan="1" colspan="1">.181</td><td rowspan="1" colspan="1">8.2</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag &#x000d7; T1 valence</td><td rowspan="1" colspan="1">8,296</td><td rowspan="1" colspan="1">.744</td><td rowspan="1" colspan="1">.010</td><td rowspan="1" colspan="1">&#x0003c;1</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">Experiment 1: Full model</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag</td><td rowspan="1" colspan="1">4,124</td><td rowspan="1" colspan="1">1.000</td><td rowspan="1" colspan="1">.505</td><td rowspan="1" colspan="1">31.6</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T1 valence</td><td rowspan="1" colspan="1">2,62</td><td rowspan="1" colspan="1">.908</td><td rowspan="1" colspan="1">.219</td><td rowspan="1" colspan="1">8.7</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T2 valence</td><td rowspan="1" colspan="1">2,62</td><td rowspan="1" colspan="1">.941</td><td rowspan="1" colspan="1">.044</td><td rowspan="1" colspan="1">1.4</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag &#x000d7; T1 valence</td><td rowspan="1" colspan="1">8,248</td><td rowspan="1" colspan="1">.870</td><td rowspan="1" colspan="1">.060</td><td rowspan="1" colspan="1">2.0</td><td rowspan="1" colspan="1">.078</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag &#x000d7; T2 valence</td><td rowspan="1" colspan="1">8,248</td><td rowspan="1" colspan="1">.972</td><td rowspan="1" colspan="1">.015</td><td rowspan="1" colspan="1">&#x0003c;1</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T1 valence &#x000d7; T2 valence</td><td rowspan="1" colspan="1">4,124</td><td rowspan="1" colspan="1">.902</td><td rowspan="1" colspan="1">.025</td><td rowspan="1" colspan="1">&#x0003c;1</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">Lag &#x000d7; T1 valence &#x000d7; T2 valence</td><td rowspan="1" colspan="1">16,496</td><td rowspan="1" colspan="1">.761</td><td rowspan="1" colspan="1">.017</td><td rowspan="1" colspan="1">&#x0003c;1</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">Experiment 2: Valence-varied T2 after neutral T1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag</td><td rowspan="1" colspan="1">9,171</td><td rowspan="1" colspan="1">.877</td><td rowspan="1" colspan="1">.288</td><td rowspan="1" colspan="1">7.7</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T2 valence</td><td rowspan="1" colspan="1">2,38</td><td rowspan="1" colspan="1">.704</td><td rowspan="1" colspan="1">.278</td><td rowspan="1" colspan="1">7.3</td><td rowspan="1" colspan="1">&#x0003c;.01</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag &#x000d7; T2 valence</td><td rowspan="1" colspan="1">18,342</td><td rowspan="1" colspan="1">.690</td><td rowspan="1" colspan="1">.041</td><td rowspan="1" colspan="1">&#x0003c;1</td><td rowspan="1" colspan="1">n.s.</td></tr><tr valign="top"><td rowspan="1" colspan="1">Experiment 3: Neutral T2 after valence-varied T1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag</td><td rowspan="1" colspan="1">9,207</td><td rowspan="1" colspan="1">.969</td><td rowspan="1" colspan="1">.419</td><td rowspan="1" colspan="1">16.6</td><td rowspan="1" colspan="1">&#x0003c;.001</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;T1 valence</td><td rowspan="1" colspan="1">2,46</td><td rowspan="1" colspan="1">.595</td><td rowspan="1" colspan="1">.092</td><td rowspan="1" colspan="1">2.3</td><td rowspan="1" colspan="1">.10</td></tr><tr valign="top"><td rowspan="1" colspan="1">&#x02003;Lag &#x000d7; T1 valence</td><td rowspan="1" colspan="1">18,414</td><td rowspan="1" colspan="1">.995</td><td rowspan="1" colspan="1">.069</td><td rowspan="1" colspan="1">1.7</td><td rowspan="1" colspan="1">&#x0003c;.05</td></tr></tbody></table></alternatives></table-wrap><fig id="fig1" position="float"><label>Figure 1</label><caption><p>Intratrial sequence: Two face targets (T1, T2) are embedded in a 10 Hz rapid visual serial presentation of scrambled faces. In this example, T2 occurs after two distractors, this is, at Lag 3. After each trial, three possible face identities are shown for T1 and T2, respectively.</p></caption><graphic id="fig1a" xlink:href="emo_14_6_1007_fig1a"/></fig><fig id="fig2" position="float"><label>Figure 2</label><caption><p>Target 2 (T2) recognition percentage after correctly recognized Target 1 (T1), for the three experiments. Analysis of variance results are summarized in <xref ref-type="table" id="tbc1-7" rid="tbl1">Table 1</xref>. L: Lag.</p></caption><graphic id="fig2a" xlink:href="emo_14_6_1007_fig2a"/></fig></floats-group></article>
