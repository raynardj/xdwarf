<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-archivearticle1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">25419662</article-id><article-id pub-id-type="publisher-id">PONE-D-13-52623</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0113198</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Applications</subject></subj-group><subj-group><subject>Computer Modeling</subject></subj-group><subj-group><subject>Computing Methods</subject></subj-group></subj-group></article-categories><title-group><article-title>Locality Constrained Joint Dynamic Sparse Representation for Local Matching Based Face Recognition</article-title><alt-title alt-title-type="running-head">Locality Constrained Joint Dynamic Sparse Representation</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Jianzhong</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Yi</surname><given-names>Yugen</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Wei</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff4">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Shi</surname><given-names>Yanjiao</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Qi</surname><given-names>Miao</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ming</given-names></name><xref ref-type="aff" rid="aff4">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Baoxue</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref><xref ref-type="aff" rid="aff5">
<sup>5</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Kong</surname><given-names>Jun</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff4">
<sup>4</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>College of Computer Science and Information Technology, Northeast Normal University, Changchun, China</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>School of Mathematics and Statistics, Northeast Normal University, Changchun, China</addr-line>
</aff><aff id="aff3">
<label>3</label>
<addr-line>National Engineering Laboratory for Druggable Gene and Protein Screening, Northeast Normal University, Changchun, China</addr-line>
</aff><aff id="aff4">
<label>4</label>
<addr-line>Key Laboratory of Intelligent Information Processing of Jilin Universities, Northeast Normal University, Changchun, China</addr-line>
</aff><aff id="aff5">
<label>5</label>
<addr-line>School of Statistics, Capital University of Economics and Business, Beijing, China</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Suarez</surname><given-names>Oscar Deniz</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Universidad de Castilla-La Mancha, Spain</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>bxzhang@nenu.edu.cn</email> (BZ); <email>kongjun@nenu.edu.cn</email> (JK)</corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Conceived and designed the experiments: JZW YGY JK BXZ. Performed the experiments: YGY WZ YJS MQ MZ. Analyzed the data: JZW YGY JK BXZ. Wrote the paper: JZW YGY.</p></fn></author-notes><pub-date pub-type="collection"><year>2014</year></pub-date><pub-date pub-type="epub"><day>24</day><month>11</month><year>2014</year></pub-date><volume>9</volume><issue>11</issue><elocation-id>e113198</elocation-id><history><date date-type="received"><day>4</day><month>1</month><year>2014</year></date><date date-type="accepted"><day>24</day><month>10</month><year>2014</year></date></history><permissions><copyright-statement>&#x000a9; 2014 Wang et al</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Wang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>Recently, Sparse Representation-based Classification (SRC) has attracted a lot of attention for its applications to various tasks, especially in biometric techniques such as face recognition. However, factors such as lighting, expression, pose and disguise variations in face images will decrease the performances of SRC and most other face recognition techniques. In order to overcome these limitations, we propose a robust face recognition method named Locality Constrained Joint Dynamic Sparse Representation-based Classification (LCJDSRC) in this paper. In our method, a face image is first partitioned into several smaller sub-images. Then, these sub-images are sparsely represented using the proposed locality constrained joint dynamic sparse representation algorithm. Finally, the representation results for all sub-images are aggregated to obtain the final recognition result. Compared with other algorithms which process each sub-image of a face image independently, the proposed algorithm regards the local matching-based face recognition as a multi-task learning problem. Thus, the latent relationships among the sub-images from the same face image are taken into account. Meanwhile, the locality information of the data is also considered in our algorithm. We evaluate our algorithm by comparing it with other state-of-the-art approaches. Extensive experiments on four benchmark face databases (ORL, Extended YaleB, AR and LFW) demonstrate the effectiveness of LCJDSRC.</p></abstract><funding-group><funding-statement>This work is supported by Fund of Jilin Provincial Science &#x00026; Technology Department (20130206042GX, 20111804), Young scientific research fund of Jilin province science and technology development project (No. 20130522115JH, 201201070, 201201063) and National Natural Science Foundation of China (No. 11271064, 61403078). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="26"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>In the past two decades, face recognition has become one of the most active and challenging research topics in pattern recognition and computer vision fields due to its wide range of applications in biometrics, human-computer interaction, information security and so on <xref rid="pone.0113198-Li1" ref-type="bibr">[1]</xref>, <xref rid="pone.0113198-Jafri1" ref-type="bibr">[2]</xref>. Although many researchers have proposed various algorithms for face recognition <xref rid="pone.0113198-Li1" ref-type="bibr">[1]</xref>, <xref rid="pone.0113198-Jafri1" ref-type="bibr">[2]</xref>, it is still a challenging problem <xref rid="pone.0113198-Zou1" ref-type="bibr">[3]</xref>, <xref rid="pone.0113198-Subban1" ref-type="bibr">[4]</xref>. This is because the appearance of real-world face images is always affected by illumination condition, aging, pose, facial expression and disguise variances. Moreover, some other problematic factors such as occlusion and noise will also impair the performances of face recognition algorithms.</p><p>Recently, sparse representation (or sparse coding) techniques have drawn wide interest and been successfully used in signal, image, video processing and biometric applications <xref rid="pone.0113198-Ramirez1" ref-type="bibr">[5]</xref>. Motivated by sparse representation, a novel face recognition method named Sparse Representation-based Classification (SRC) <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref> was proposed by Wright et al. In SRC, a query image is first sparsely linear coded by the original training images, and then the classification is performed by checking which class leads to the minimal representation residual of the query image. Since the experimental results in Wright et al.'s pioneer work showed that the SRC achieved impressive face recognition performance, the research of sparse representation-based face recognition was largely boosted and lots of algorithms have been developed. Gao et al. <xref rid="pone.0113198-Gao1" ref-type="bibr">[7]</xref> proposed an extension of SRC named kernel SRC (KSRC), which performed the sparse representation technique in a new high-dimensional feature space obtained by the kernel trick <xref rid="pone.0113198-ShaweTaylor1" ref-type="bibr">[8]</xref>. Meanwhile, Yang et al. <xref rid="pone.0113198-Yang1" ref-type="bibr">[9]</xref> utilized Gabor features rather than the original facial features in SRC to improve recognition accuracy. Wang et al. <xref rid="pone.0113198-Wang1" ref-type="bibr">[10]</xref> developed a Locality constrained Linear Coding (LLC) scheme. In LLC, the query image is represented only using the nearest codewords (or training samples). However, SRC, KSRC and LLC did not take the structure of the training data into consideration. Thus, their methods may fail to deal with the data which lie on multiple low-dimensional subspaces in the high-dimensional ambient space <xref rid="pone.0113198-Roweis1" ref-type="bibr">[11]</xref>, <xref rid="pone.0113198-Tenenbaum1" ref-type="bibr">[12]</xref>. In order to overcome this limitation, Elhamifar et al. put forward a structured sparse representation algorithm in <xref rid="pone.0113198-Elhamifar1" ref-type="bibr">[13]</xref>. The main idea of their algorithm is to find a good representation of the query sample using the minimum number of structure blocks in the training set. In <xref rid="pone.0113198-Wagner1" ref-type="bibr">[14]</xref>, Wagner et al. proposed a sparse representation-based method that could deal with face misalignment and illumination variation. Yang et al. introduced a Robust Sparse Coding model (RSC) in <xref rid="pone.0113198-Yang2" ref-type="bibr">[15]</xref>. RSC relaxed the assumption that the representation residual should follow the Gaussian or Laplacian distribution in original SRC, and sought for a maximum likelihood estimator (MLE) solution for the sparse coding problem. Moreover, Deng et al. <xref rid="pone.0113198-Deng1" ref-type="bibr">[16]</xref> proposed another extended SRC (ESRC) method in which they assumed the intra-class variations of one subject can be approximately represented by a sparse linear combination of the other subjects. Therefore, ESRC can successfully handle face recognition with limited training samples per subject. Recently, Mi et al. <xref rid="pone.0113198-Mi1" ref-type="bibr">[17]</xref> presented a novel face recognition method named sparse representation-based classification on <italic>k</italic>-nearest subspace (SRC-KNS). In SRC-KNS, the distance between the test image and the subspace of each individual class is first exploited to determine the <italic>k</italic> nearest subspaces, and then the SRC is performed on the <italic>k</italic> selected classes.</p><p>Although the above-mentioned SRC-based methods perform well, their recognition performances may also be affected by some problematic factors (such as illumination, expression, disguises and pose) in real-world face images <xref rid="pone.0113198-Chen1" ref-type="bibr">[18]</xref>. The main reason is that they utilize the holistic information of the face images for recognition. Based on the observation that some of the local facial features would not vary with pose, lighting, facial expression and disguise, some local matching-based methods which extract facial features from different levels of locality have been proposed showing more promising results in face recognition tasks <xref rid="pone.0113198-Zou1" ref-type="bibr">[3]</xref>, <xref rid="pone.0113198-Pentland1" ref-type="bibr">[19]</xref>&#x02013;<xref rid="pone.0113198-Wang3" ref-type="bibr">[25]</xref>. In <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>, Wright et al. also incorporated their SRC into the local matching framework to improve its performance. In their local matching-based SRC (LMSRC), the query and training face images are first divided into a number of equally sized sub-images. Then, each sub-image of the query face is represented by the corresponding sub-images of the training set using SRC, and a final decision is made by majority voting for classification. However, since LMSRC represented the sub-images independently, it merely focused on how to sparsely encode each sub-image of the query face but ignored the latent relationships among the multiple sub-images from the same face image, which may weaken its recognition performance <xref rid="pone.0113198-Wang3" ref-type="bibr">[25]</xref>, <xref rid="pone.0113198-Sinha1" ref-type="bibr">[26]</xref>.</p><p>In the local matching-based face recognition framework, each sub-image of a face can be regarded as a sub-pattern which contains a partial feature of the face image. Furthermore, different sub-images divided from the same image can reflect various kinds of information of the query face, and have some latent connections with each other since they jointly provide the full information of the whole face image. Therefore, jointly estimating the sparse representation models of latently-related sub-images from a query face image can be viewed as a &#x0201c;multi-task learning&#x0201d; problem in which each sub-image is a task <xref rid="pone.0113198-Yang3" ref-type="bibr">[27]</xref>. Nowadays, some multi-task learning-based sparse representation algorithms have been proposed to take advantage of an object's different features. Yuan et al. proposed a Multi-task Joint Sparse Representation-based Classification method (MTJSRC) in <xref rid="pone.0113198-Yuan1" ref-type="bibr">[28]</xref>. MTJSRC assumes that the sparse representation coefficients of different features have the same sparsity pattern. Thus, the <italic>&#x02113;</italic>
<sub>1,2</sub>-norm is utilized in this method to make the sparse representation coefficients of different features be the same at atom-level. However, this assumption is too strict to hold in practice. For example, if the appearance of a face image is affected by large illumination changes, it would be hard to represent all sub-images of this face properly by the same set of atoms. Therefore, Zhang et al. proposed a new method named Joint Dynamic Sparse Representation-based Classification (JDSRC) to deal with this problem <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>. In their method, a novel concept of joint dynamic sparsity is introduced to represent the different features of an object by different sets of training samples from the same class <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>. As a result, the sparse representation coefficients of different features obtained by JDSRC tend to have the same sparsity pattern at class-level rather than atom-level. Additionally, another method named Relaxed Collaborative Representation (RCR) was proposed by Yang et al. <xref rid="pone.0113198-Yang3" ref-type="bibr">[27]</xref>. RCR assumes the sparse representation coefficients with respect to different features should be alike. Therefore, the sparse representation coefficients of all features obtained by RCR have the similar sparsity pattern in appearance (i.e. the positions and values of non-zero elements in different representation coefficient vectors are similar to each other). Though the experiments in <xref rid="pone.0113198-Yang3" ref-type="bibr">[27]</xref>&#x02013;<xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref> showed that the MTJSRC, JDSRC and RCR algorithms achieved better recognition and classification performances than SRC, the locality information (i.e. similarity between the query and training samples) <xref rid="pone.0113198-Wang1" ref-type="bibr">[10]</xref> was neglected in all of them. Therefore, these algorithms may select the training samples which are dissimilar to the query sub-image for representation and produce unsatisfying recognition results. In recent studies, since the researchers have shown that exploiting locality information of data was more essential than sparsity in some cases <xref rid="pone.0113198-Chao1" ref-type="bibr">[30]</xref>&#x02013;<xref rid="pone.0113198-Yu1" ref-type="bibr">[32]</xref>, it is crucial to incorporate the locality information into the multi-task learning-based sparse representation algorithms.</p><p>Inspired by the pioneer work of joint sparse representation and the importance of data locality, we present a novel multi-task learning-based sparse representation algorithm called Locality Constrained Joint Dynamic Sparse Representation-based Classification (LCJDSRC) for local matching-based face recognition. One important advantage of the proposed algorithm is that it explicitly integrates the joint sparse representation and the locality constraint into a unified framework. Therefore, our algorithm not only takes the latent correlations among different local facial features into account but also considers the similarity between the query and training samples. Like the existing JDSRC, the sparse representation coefficients of different sub-images from a face have the same sparsity pattern at class-level in LCJDSRC. However, since the locality constraint in our algorithm will magnify the representation coefficients corresponding to the similar samples of the query sub-image while reducing the dissimilar ones, LCJDSRC tends to select the nearest neighbors of the query sub-image for representation to improve its recognition performance. The effectiveness of our algorithm is evaluated by extensive experiments on four well-known face databases and compared with other state-of-the-art approaches.</p><p>The rest of this paper is organized as follows. The LCJDSRC model is presented in Section 2. In Section 3, the proposed algorithm and several other methods are evaluated on the four databases (ORL, Extended YaleB, AR and LFW). Finally, the conclusions are given in Section 4.</p></sec><sec id="s2"><title>Locality Constrained Joint Dynamic Sparse Representation</title><p>In this section, we first present the outline of the proposed local matching-based face recognition method. Then, the details of our Locality Constrained Joint Dynamic Sparse Representation algorithm are discussed. Next, the recognition criterion of our algorithm is given. Finally, some comparisons between the proposed algorithm and related work are also analyzed.</p><sec id="s2a"><title>Outline</title><p>There are four steps in the proposed local matching-based face recognition algorithm. The first step is to partition the query face image and the face images in the training set. Generally, there are two different techniques to implement the partition, i.e., local components and local regions. Local components are areas occupied by the facial components, such as eyes, nose and mouth, and local regions are local sub-images centered at designated coordinates of a common coordinate system. Since some researchers have verified that the local region partition is to be preferred in local matching-based face recognition <xref rid="pone.0113198-Zou1" ref-type="bibr">[3]</xref>, we adopt rectangular regions to partition the images as many other approaches do <xref rid="pone.0113198-Chen2" ref-type="bibr">[20]</xref>&#x02013;<xref rid="pone.0113198-Wang3" ref-type="bibr">[25]</xref>. That is, the query and training face images are divided into several smaller rectangular sub-images in our algorithm. In the second step, the sub-images of the query face are sparsely represented by their corresponding sub-images in the training set using the proposed locality constrained joint dynamic sparse representation (LCJDSR) algorithm, which not only considers the latent relationships among the sub-images but also takes the locality information into account. The third step of our algorithm is to compute the representation residual of each sub-image using the sparse representation coefficients obtained by LCJDSR. At last, the total representation residuals of all sub-images from the query face are aggregated for final recognition. The flow diagram of the proposed algorithm can be seen in <xref ref-type="fig" rid="pone-0113198-g001">Figure 1</xref>.</p><fig id="pone-0113198-g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.g001</object-id><label>Figure 1</label><caption><title>The diagram of the proposed algorithm.</title><p>The face images are collected from the Extended YaleB database <xref rid="pone.0113198-Georghiades1" ref-type="bibr">[36]</xref>.</p></caption><graphic xlink:href="pone.0113198.g001"/></fig></sec><sec id="s2b"><title>Locality Constrained Joint Dynamic Sparse Representation Model</title><p>Let <inline-formula><inline-graphic xlink:href="pone.0113198.e001.jpg"/></inline-formula>denote <italic>N</italic> face images belonging to <italic>C</italic> persons in the training set (<italic>N<sub>c</sub></italic> samples, <inline-formula><inline-graphic xlink:href="pone.0113198.e002.jpg"/></inline-formula> are associated to each person), and let the size of each face image be <inline-formula><inline-graphic xlink:href="pone.0113198.e003.jpg"/></inline-formula>. Given a query face image <inline-formula><inline-graphic xlink:href="pone.0113198.e004.jpg"/></inline-formula>, we partition it into <italic>M</italic> non-overlapping sub-images and then concatenate each sub-image into a column vector. Thus, <italic>y</italic> can be represented as <inline-formula><inline-graphic xlink:href="pone.0113198.e005.jpg"/></inline-formula> in which <inline-formula><inline-graphic xlink:href="pone.0113198.e006.jpg"/></inline-formula> is the vector of the <italic>i-</italic>th sub-image. Similarly, the face images in the training set are also partitioned into <inline-formula><inline-graphic xlink:href="pone.0113198.e007.jpg"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="pone.0113198.e008.jpg"/></inline-formula> is the set which contains the <italic>i-</italic>th sub-image vectors of all training samples. Here, <italic>d<sub>i</sub></italic> is the dimension of the <italic>i-</italic>th sub-images in <italic>y</italic> and <inline-formula><inline-graphic xlink:href="pone.0113198.e009.jpg"/></inline-formula>.</p><p>After partition the query and training face images into non-overlapping sub-images, our objective is to sparsely represent the sub-images in <italic>y</italic> using their corresponding sub-images in the training set. To address this problem, one can simply apply the standard SRC to each of the <italic>M</italic> sub-images in <italic>y</italic>, which can be written as:<disp-formula id="pone.0113198.e010"><graphic xlink:href="pone.0113198.e010.jpg" position="anchor" orientation="portrait"/><label>(1)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e011.jpg"/></inline-formula> is the coefficient vector of the <italic>m-</italic>th sub-image and <inline-formula><inline-graphic xlink:href="pone.0113198.e012.jpg"/></inline-formula> is a tradeoff parameter. Taking all <italic>M</italic> sub-images into account, the objective function of local matching-based SRC is:<disp-formula id="pone.0113198.e013"><graphic xlink:href="pone.0113198.e013.jpg" position="anchor" orientation="portrait"/><label>(2)</label></disp-formula>
</p><p>In fact, this strategy is the same as Wright et al.'s <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>. Therefore, as we have discussed in Section 1, it is far from optimal due to the following reasons. First, the objective function in <xref ref-type="disp-formula" rid="pone.0113198.e013">Equation (2</xref>) neglects the similarity between the query and the training samples, thus it may select training sub-images which are not similar to the query face for representation. Second, <xref ref-type="disp-formula" rid="pone.0113198.e013">Equation (2</xref>) represents each sub-image of the query face independently, ignoring the latent relationships among the sub-images.</p><p>In SRC and other related algorithms, the classification result of a query sample is always determined by the class-wise minimum representation residual <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>&#x02013;<xref rid="pone.0113198-Gao1" ref-type="bibr">[7]</xref>, <xref rid="pone.0113198-Yang1" ref-type="bibr">[9]</xref>-<xref rid="pone.0113198-Wang1" ref-type="bibr">[10]</xref>, <xref rid="pone.0113198-Elhamifar1" ref-type="bibr">[13]</xref>&#x02013;<xref rid="pone.0113198-Mi1" ref-type="bibr">[17]</xref>, so it is reasonable to believe that selecting training samples similar to the query image for representation can improve classification and recognition <xref rid="pone.0113198-Chao1" ref-type="bibr">[30]</xref>, <xref rid="pone.0113198-Wei1" ref-type="bibr">[31]</xref>. According to the above analysis, a locality adaptor which measures the similarity between the query and training samples is introduced in this study to overcome the first shortcoming of <xref ref-type="disp-formula" rid="pone.0113198.e013">Equation (2</xref>). The locality adaptor is defined as:<disp-formula id="pone.0113198.e014"><graphic xlink:href="pone.0113198.e014.jpg" position="anchor" orientation="portrait"/><label>(3)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e015.jpg"/></inline-formula> is a parameter which determines the decay rate of the weight function, <inline-formula><inline-graphic xlink:href="pone.0113198.e016.jpg"/></inline-formula> is the <italic>m-</italic>th sub-image of the query face and <inline-formula><inline-graphic xlink:href="pone.0113198.e017.jpg"/></inline-formula> denotes the <italic>m-</italic>th sub-image of the <italic>j-</italic>th training sample. From <xref ref-type="disp-formula" rid="pone.0113198.e014">Equation (3</xref>), it is clear that a smaller <inline-formula><inline-graphic xlink:href="pone.0113198.e018.jpg"/></inline-formula> indicates that <inline-formula><inline-graphic xlink:href="pone.0113198.e019.jpg"/></inline-formula> is more similar to the query sub-image <inline-formula><inline-graphic xlink:href="pone.0113198.e020.jpg"/></inline-formula>, and vice versa.</p><p>In order to overcome the second problem of <xref ref-type="disp-formula" rid="pone.0113198.e013">Equation (2</xref>), a dynamic active set is adopted in our algorithm. The concept of dynamic active set was first proposed by Zhang et al. to exploit the correlations (or relationships) among the multiple observations which describe the same subject during the multi-task learning-based sparse representation. In <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>, a dynamic active set is defined as a set of coefficient indices belonging to the same class, and a number of dynamic active sets are jointly activated to sparsely represent the multiple observations. Formally, let <inline-formula><inline-graphic xlink:href="pone.0113198.e021.jpg"/></inline-formula> be the matrix containing the sparse representation coefficients of <italic>M</italic> sub-images from <italic>y</italic>, where <inline-formula><inline-graphic xlink:href="pone.0113198.e022.jpg"/></inline-formula> is the coefficient vector of the <italic>m-</italic>th sub-image. Then, each dynamic active set (denoted by <inline-formula><inline-graphic xlink:href="pone.0113198.e023.jpg"/></inline-formula>) can be described as a set of row indices of coefficients whose corresponding samples in the training set are from the same class. To promote sparsity and allow only a small number of dynamic active sets to be involved during the joint sparsity representation, a mixed-norm which applies <italic>&#x02113;<sub>2</sub></italic>-norm on each dynamic active set and then <italic>&#x02113;</italic>
<sub>0</sub>-norm across the <italic>&#x02113;</italic>
<sub>2</sub>-norm is defined as: <disp-formula id="pone.0113198.e024"><graphic xlink:href="pone.0113198.e024.jpg" position="anchor" orientation="portrait"/><label>(4)</label></disp-formula> where<disp-formula id="pone.0113198.e025"><graphic xlink:href="pone.0113198.e025.jpg" position="anchor" orientation="portrait"/><label>(5)</label></disp-formula> is the vector formed by the coefficients associated with the <italic>s-</italic>th dynamic active set <inline-formula><inline-graphic xlink:href="pone.0113198.e026.jpg"/></inline-formula>, in which <inline-formula><inline-graphic xlink:href="pone.0113198.e027.jpg"/></inline-formula> is the row index of the selected training sample for the <italic>m-</italic>th column of <italic>A</italic> in the <italic>s-</italic>th dynamic active set. In order to better illustrate the organization of the dynamic active set, an example is provided in <xref ref-type="fig" rid="pone-0113198-g002">Figure 2</xref>. From <xref ref-type="fig" rid="pone-0113198-g002">Figure 2</xref>, we can see that there are two dynamic active sets denoted by <inline-formula><inline-graphic xlink:href="pone.0113198.e028.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e029.jpg"/></inline-formula> in this example. Therefore, according to the definition of the dynamic active set, we can get <inline-formula><inline-graphic xlink:href="pone.0113198.e030.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e031.jpg"/></inline-formula>. Furthermore, according to <xref ref-type="disp-formula" rid="pone.0113198.e025">Equation (5</xref>), the coefficient vectors associated with <inline-formula><inline-graphic xlink:href="pone.0113198.e032.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e033.jpg"/></inline-formula> are <inline-formula><inline-graphic xlink:href="pone.0113198.e034.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e035.jpg"/></inline-formula>, respectively. For more details about the dynamic active set, the readers can refer to <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>.</p><fig id="pone-0113198-g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.g002</object-id><label>Figure 2</label><caption><title>An example of the organization of the dynamic active set.</title><p>For simplicity, we just take two classes as an example and the number of sub-images <italic>M</italic> is 4. <italic>A</italic>&#x0200a;=&#x0200a;[<italic>A</italic>
<sup>1</sup>,<italic>A</italic>
<sup>2</sup>,<italic>A</italic>
<sup>3</sup>,<italic>A</italic>
<sup>4</sup>] is a coefficient matrix and <italic>A<sup>i</sup></italic> (<italic>i</italic>&#x0200a;=&#x0200a;1,2,3,4) is a column vector of <italic>A</italic>, the number in each squared block denotes the sparse representation coefficient and the blank blocks denote zero values.</p></caption><graphic xlink:href="pone.0113198.g002"/></fig><p>Now, by integrating the locality constraint and the joint sparse representation into a unified framework, we can obtain the objective function of our model as follow<disp-formula id="pone.0113198.e036"><graphic xlink:href="pone.0113198.e036.jpg" position="anchor" orientation="portrait"/><label>(6)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e037.jpg"/></inline-formula> is a parameter to control the tradeoff between the two terms in <xref ref-type="disp-formula" rid="pone.0113198.e036">Equation (6</xref>), &#x02299;denotes the element-wise multiplication, <inline-formula><inline-graphic xlink:href="pone.0113198.e038.jpg"/></inline-formula> is the locality adaptor vector of the <italic>m-</italic>th sub-image and <italic>K</italic> is the sparsity level which denotes the number of non-zero elements in each <inline-formula><inline-graphic xlink:href="pone.0113198.e039.jpg"/></inline-formula>
<xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>, <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>.</p><p>In the proposed algorithm, since we want to guarantee that the sub-images of a query face can be well represented by their corresponding samples in the training set, the first term in <xref ref-type="disp-formula" rid="pone.0113198.e036">Equation (6</xref>) which stands for the representation residual should be minimized. On the other hand, the second term is referred to as locality constraint because minimizing this term will magnify the absolute values of coefficients corresponding to the training samples similar to the query sub-image and reduce the dissimilar ones. Furthermore, the mix-norm regularization term <inline-formula><inline-graphic xlink:href="pone.0113198.e040.jpg"/></inline-formula> combines the cues from all the sub-images coming from the query image <italic>y</italic> during the representation process and promotes a joint sparsity pattern shared at class-level <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>.</p></sec><sec id="s2c"><title>The optimization stage</title><p>Since the regularization term in our proposed model contains the <italic>&#x02113;</italic>
<sub>0</sub>-norm, how to solve <xref ref-type="disp-formula" rid="pone.0113198.e036">Equation (6</xref>) becomes a challenging problem. In this subsection, a greedy algorithm based on Matching Pursuit (MP) <xref rid="pone.0113198-Mallat1" ref-type="bibr">[33]</xref> is presented to optimize the objective function in <xref ref-type="disp-formula" rid="pone.0113198.e036">Equation (6</xref>). In our algorithm, we initialize the representation residual of each query sub-image as <inline-formula><inline-graphic xlink:href="pone.0113198.e041.jpg"/></inline-formula> and the selected dynamic active sets <italic>I</italic>
<sub>0</sub> as empty. Then, the following four steps are processed in the <italic>t-</italic>th iteration (<inline-formula><inline-graphic xlink:href="pone.0113198.e042.jpg"/></inline-formula>) until certain conditions are satisfied.</p><p>
<bold>Step 1.</bold> Select new candidates based on the current residual.</p><p>Based on the current representation residual, some candidate dynamic active sets are selected. First, the representation coefficients of each query sub-image are computed by the inner product of the representation residual and its corresponding training set as:<disp-formula id="pone.0113198.e043"><graphic xlink:href="pone.0113198.e043.jpg" position="anchor" orientation="portrait"/><label>(7)</label></disp-formula>
</p><p>Then, according to the representation coefficient matrix <inline-formula><inline-graphic xlink:href="pone.0113198.e044.jpg"/></inline-formula>, <italic>L</italic> candidate dynamic active sets whose associated coefficients can best approximate to<inline-formula><inline-graphic xlink:href="pone.0113198.e045.jpg"/></inline-formula>are selected. According to the suggestions in <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref> and <xref rid="pone.0113198-Duarte1" ref-type="bibr">[34]</xref>, we set <italic>L</italic>&#x0200a;=&#x0200a;2<italic>K</italic> in this study. This problem can be solved by the following objective function:<disp-formula id="pone.0113198.e046"><graphic xlink:href="pone.0113198.e046.jpg" position="anchor" orientation="portrait"/><label>(8)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e047.jpg"/></inline-formula> is a sparse matrix which only keeps the coefficients associated with the selected candidate dynamic active sets in <inline-formula><inline-graphic xlink:href="pone.0113198.e048.jpg"/></inline-formula> and sets the other coefficients to be zero. The solution of <xref ref-type="disp-formula" rid="pone.0113198.e046">Equation (8</xref>) can be obtained by the Joint Dynamic Sparsity mapping (JDS mapping) detailed in <xref ref-type="fig" rid="pone-0113198-g003">Figure 3</xref>, which gives the dynamic active sets and associated <inline-formula><inline-graphic xlink:href="pone.0113198.e049.jpg"/></inline-formula> based on the input coefficient matrix <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>. In JDS mapping, one dynamic active set is selected in each of its iteration by four steps. Firstly, the maximum absolute coefficient for each class and each sub-image is calculated by Equation (9). Then, these maximum absolute coefficients are combined across the sub-images for each class as the total response by Equation (10). Third, one of the dynamic active sets which gives the maximum total response is selected by Equation (11). At last, the selected dynamic active sets are added into the matrix <italic>I<sub>t</sub></italic> as a row and its associated coefficients in <inline-formula><inline-graphic xlink:href="pone.0113198.e050.jpg"/></inline-formula> are assigned to <inline-formula><inline-graphic xlink:href="pone.0113198.e051.jpg"/></inline-formula> by Equation (12) and (13). In order to ensure that the selected dynamic active set will not be selected again in the following iterations, we also set its associated coefficients in <inline-formula><inline-graphic xlink:href="pone.0113198.e052.jpg"/></inline-formula> to be zero by Equation (14). These four steps are iterated until the desired number of dynamic active sets is obtained. A simple example about the organizations of matrices <italic>A</italic>, <italic>I</italic> and <inline-formula><inline-graphic xlink:href="pone.0113198.e053.jpg"/></inline-formula> in JDS mapping can be seen in <xref ref-type="fig" rid="pone-0113198-g004">Figure 4</xref>.</p><fig id="pone-0113198-g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.g003</object-id><label>Figure 3</label><caption><title>The joint dynamic sparsity mapping algorithm.</title></caption><graphic xlink:href="pone.0113198.g003"/></fig><fig id="pone-0113198-g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.g004</object-id><label>Figure 4</label><caption><title>An example about the matrix organization in JDS mapping (<italic>L</italic>&#x0200a;=&#x0200a;2).</title><p>(a) the input representation coefficient matrix <italic>A</italic>; (b) the output matrix <italic>I</italic> contains two selected candidate dynamic active sets selected by JDS mapping; (c) the output sparse matrix <inline-formula><inline-graphic xlink:href="pone.0113198.e054.jpg"/></inline-formula> associated with <italic>I</italic>.</p></caption><graphic xlink:href="pone.0113198.g004"/></fig><p>
<bold>Step 2.</bold> Merge the newly selected candidates with the previously selected sets.</p><p>After obtaining the matrix <italic>I<sub>t</sub></italic> which contains the <italic>L</italic> candidate dynamic active sets selected by Algorithm 1, we merge it with <italic>I<sub>t</sub></italic>
<sub>-1</sub> to update the dynamic active sets:<disp-formula id="pone.0113198.e055"><graphic xlink:href="pone.0113198.e055.jpg" position="anchor" orientation="portrait"/><label>(15)</label></disp-formula>
</p><p>
<bold>Step 3.</bold> Estimate the representation coefficients based on the merged set.</p><p>Let <inline-formula><inline-graphic xlink:href="pone.0113198.e056.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e057.jpg"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="pone.0113198.e058.jpg"/></inline-formula> is the vector diagonalization operator and <inline-formula><inline-graphic xlink:href="pone.0113198.e059.jpg"/></inline-formula> is an operator that only keeps the columns whose indices are included in<inline-formula><inline-graphic xlink:href="pone.0113198.e060.jpg"/></inline-formula>while setting others to be zero vectors in matrices <inline-formula><inline-graphic xlink:href="pone.0113198.e061.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e062.jpg"/></inline-formula>. The representation coefficient of each sub-image can be updated by<disp-formula id="pone.0113198.e063"><graphic xlink:href="pone.0113198.e063.jpg" position="anchor" orientation="portrait"/><label>(16)</label></disp-formula>
</p><p>More details about the derivation process of <xref ref-type="disp-formula" rid="pone.0113198.e063">Equation (16</xref>) can be seen in the in the supporting information <xref ref-type="supplementary-material" rid="pone.0113198.s001">file S1</xref>.</p><p>
<bold>Step 4.</bold> Prune the merged set to a specified sparsity level based on the newly estimated representation coefficients.</p><p>Based on the representation coefficients <inline-formula><inline-graphic xlink:href="pone.0113198.e064.jpg"/></inline-formula> obtained from <xref ref-type="disp-formula" rid="pone.0113198.e063">Equation (16</xref>), the <italic>K</italic> most representative dynamic active sets are calculated using JDS mapping. Therefore, the selected dynamic active sets are further updated by<disp-formula id="pone.0113198.e065"><graphic xlink:href="pone.0113198.e065.jpg" position="anchor" orientation="portrait"/><label>(17)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e066.jpg"/></inline-formula> is the matrix which contains the <italic>K</italic> most representative dynamic active sets obtained by JDS mapping. That is, only <italic>K</italic> dynamic active sets in <inline-formula><inline-graphic xlink:href="pone.0113198.e067.jpg"/></inline-formula> are selected according to <inline-formula><inline-graphic xlink:href="pone.0113198.e068.jpg"/></inline-formula> and others are pruned away in this step.</p><p>
<bold>Step 5.</bold> Update the residual.</p><p>Firstly, according to the dynamic active sets <inline-formula><inline-graphic xlink:href="pone.0113198.e069.jpg"/></inline-formula> obtained by Step4, the representation coefficient of each sub-image is further obtained by <xref ref-type="disp-formula" rid="pone.0113198.e063">Equation (16</xref>). Then, the representation residual of each sub-image is updated by<disp-formula id="pone.0113198.e070"><graphic xlink:href="pone.0113198.e070.jpg" position="anchor" orientation="portrait"/><label>(18)</label></disp-formula>
</p><p>
<bold>Step 6.</bold> Check whether the termination condition is satisfied.</p><p>The termination condition of our algorithm can be defined in two alternative ways. That is, if the predetermined maximum iteration number is reached, or the difference between the representation residuals in adjacent iterations is smaller than a preset value, the algorithm will stop. The flowchart of the proposed optimization algorithm is illustrated in <xref ref-type="fig" rid="pone-0113198-g005">Figure 5</xref>.</p><fig id="pone-0113198-g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.g005</object-id><label>Figure 5</label><caption><title>The flowchart of the proposed optimization algorithm.</title></caption><graphic xlink:href="pone.0113198.g005"/></fig></sec><sec id="s2d"><title>Recognition criterion</title><p>After obtaining the sparse representation matrix <inline-formula><inline-graphic xlink:href="pone.0113198.e071.jpg"/></inline-formula>, we combine the residuals of all the sub-images in <italic>y</italic> and get the identity of the query face as:<disp-formula id="pone.0113198.e072"><graphic xlink:href="pone.0113198.e072.jpg" position="anchor" orientation="portrait"/><label>(19)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e073.jpg"/></inline-formula> is the subset of <inline-formula><inline-graphic xlink:href="pone.0113198.e074.jpg"/></inline-formula> belonging to the <italic>i-</italic>th class, and <inline-formula><inline-graphic xlink:href="pone.0113198.e075.jpg"/></inline-formula> is the coefficient vector of <inline-formula><inline-graphic xlink:href="pone.0113198.e076.jpg"/></inline-formula> with respect to the <italic>i-</italic>th class <inline-formula><inline-graphic xlink:href="pone.0113198.e077.jpg"/></inline-formula>.</p></sec><sec id="s2e"><title>Comparisons with other works</title><p>In this subsection, the proposed algorithm is compared with other related works to demonstrate its novelty.</p><p>Firstly, the objective function of the proposed algorithm is compared with SRC <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>, LMSRC <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>, LLC <xref rid="pone.0113198-Wang1" ref-type="bibr">[10]</xref>, MTJSRC <xref rid="pone.0113198-Yuan1" ref-type="bibr">[28]</xref> and JDSRC <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>. Since all these methods adopt the sparse representation-based scheme to classify the query samples, it is the regularization on the representation coefficients that makes them different from each other. Specifically, the objective functions of these methods can be written as:<disp-formula id="pone.0113198.e078"><graphic xlink:href="pone.0113198.e078.jpg" position="anchor" orientation="portrait"/><label>(20)</label></disp-formula> where <inline-formula><inline-graphic xlink:href="pone.0113198.e079.jpg"/></inline-formula> is the <italic>m-</italic>th sub-image of the query face, <inline-formula><inline-graphic xlink:href="pone.0113198.e080.jpg"/></inline-formula> is the <italic>m-</italic>th sub-images of all training samples, <inline-formula><inline-graphic xlink:href="pone.0113198.e081.jpg"/></inline-formula> is a regularization term over the representation coefficients <inline-formula><inline-graphic xlink:href="pone.0113198.e082.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e083.jpg"/></inline-formula> is a tradeoff parameter. When the number of sub-patterns <italic>M</italic> is set to 1, <xref ref-type="disp-formula" rid="pone.0113198.e078">Equation (20</xref>) reduces to the holistic classification algorithm. In this case, if we utilize <italic>&#x02113;</italic>
<sub>1</sub>-norm and <inline-formula><inline-graphic xlink:href="pone.0113198.e084.jpg"/></inline-formula> to regularize the representation coefficients, then <xref ref-type="disp-formula" rid="pone.0113198.e078">Equation (20</xref>) becomes the standard SRC and LLC. When the number of sub-patterns <italic>M</italic> is larger than 1, if the <italic>&#x02113;</italic>
<sub>1</sub>-norm, <italic>&#x02113;</italic>
<sub>1,2</sub>-norm and<inline-formula><inline-graphic xlink:href="pone.0113198.e085.jpg"/></inline-formula>are employed to regularize the representation coefficients, <xref ref-type="disp-formula" rid="pone.0113198.e078">Equation (20</xref>) naturally becomes LMSRC <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>, MTJSRC <xref rid="pone.0113198-Yuan1" ref-type="bibr">[28]</xref> and JDSRC <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>, respectively. For our algorithm, the regularization <inline-formula><inline-graphic xlink:href="pone.0113198.e086.jpg"/></inline-formula> in <xref ref-type="disp-formula" rid="pone.0113198.e078">Equation (20</xref>) is <inline-formula><inline-graphic xlink:href="pone.0113198.e087.jpg"/></inline-formula>. The differences among LMSRC, MTJSRC, JDSRC and LCJDSRC are illustrated in <xref ref-type="fig" rid="pone-0113198-g006">Figure 6</xref>. In this figure, the rectangles denote the sub-images of a query face belonging to Class 2, and the triangles and circles represent the training sub-images belonging to Class 1 and Class 2, respectively. From <xref ref-type="fig" rid="pone-0113198-g006">Figure 6a</xref>, it can be seen that, since LMSRC simply utilizes the <italic>&#x02113;</italic>
<sub>1</sub>-norm to regularize the representation coefficients, the query sub-images are sparsely represented independently and the representation coefficient vectors (i.e. <italic>A</italic>
<sup>1</sup>,&#x02026;, <italic>A<sup>M</sup></italic>) obtained by LMSRC are very different from one another. In MTJSRC, the latent relationships of the sub-images from the query face are considered by <italic>&#x02113;</italic>
<sub>1,2</sub>-norm regularization. Thus, as demonstrated in <xref ref-type="fig" rid="pone-0113198-g006">Figure 6b</xref>, if one sub-image of a training face is selected to represent its corresponding sub-image of the query face, then the other sub-images of the same training face will also be selected to represent their corresponding sub-images in <italic>y</italic>. This leads the representation coefficient vectors of different query sub-images obtained by MTJSRC to have the same sparsity pattern at atom-level (i.e. the non-zero elements of different coefficient vectors are located in the same row). For JDSRC and LCJDSRC, since both of them take the latent relationships of the query sub-images into account by employing the mixed-norm of dynamic active set as regularization, the sparse representation coefficient vectors of different query sub-images obtained by these two algorithms have the same sparsity pattern at class-level as shown in <xref ref-type="fig" rid="pone-0113198-g006">Figure 6c</xref> and <xref ref-type="fig" rid="pone-0113198-g006">Figure 6d</xref>. That is, the non-zero elements in different coefficient vectors joined by each line (i.e. dynamic active set) are from the same class. Furthermore, since the locality information is neglected in LMSRC, MTJSRC and JDSRC, we can find that some distant training samples belonging to Class 1 are selected for representation by these three algorithms in <xref ref-type="fig" rid="pone-0113198-g006">Figure 6a-c</xref>, which may result in misrecognition. However, this limitation is overcome by the locality constraint in LCJDSRC. From <xref ref-type="fig" rid="pone-0113198-g006">Figure 6d</xref>, it can be seen that by taking the similarity between the query and training sub-images into consideration, the proposed algorithm tends to assign non-zero coefficients to similar training samples within the local neighborhoods of the query sub-images. Therefore, the training sub-images selected for representation by LCJDSRC are mostly from the same class of the query images and the recognition performance can be improved.</p><fig id="pone-0113198-g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.g006</object-id><label>Figure 6</label><caption><title>Comparisons among different algorithms.</title><p>(a) LMSRC, (b) MTJSRC, (c) JDSRC and (d) LCJDSRC. The training sub-images highlighted in red are those selected to represent the query sub-images. In the sparse representation coefficient vectors, the elements marked as "Class 1" and "Class 2" represent the coefficients corresponding to the training samples belonging to each class; the non-zero elements in the vectors are highlighted in colors.</p></caption><graphic xlink:href="pone.0113198.g006"/></fig><p>Then, the differences between the optimization algorithms in our study and other works are analyzed. Though the optimization algorithm presented in Section 2.3 looks similar to those in CoSOMP <xref rid="pone.0113198-Duarte1" ref-type="bibr">[34]</xref> and JDSRC <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>, there are two key different points between them. First, the dynamic active sets in our algorithm are obtained by Joint Dynamic Sparsity mapping, thus, our algorithm can jointly represent the sub-images of the query face and make the sparsity of the representation coefficients for different sub-images be the same at class level, which is the major difference between our algorithm and CoSOMP. Second, our algorithm updates the representation coefficients using <xref ref-type="disp-formula" rid="pone.0113198.e063">Equation (16</xref>). Therefore, the similarity between the query and training samples is considered. However, JDSRC updates the coefficients using standard least squares regression. Thus, LCJDSRC can achieve better recognition result than the JDSRC algorithm.</p></sec></sec><sec id="s3"><title>Experimental Results and Analysis</title><p>In this section, extensive experiments are conducted to verify the effectiveness of the proposed algorithm on four benchmark face databases including ORL <xref rid="pone.0113198-Samaria1" ref-type="bibr">[35]</xref>, Extended YaleB <xref rid="pone.0113198-Georghiades1" ref-type="bibr">[36]</xref>, AR <xref rid="pone.0113198-Martinez1" ref-type="bibr">[37]</xref> and LFW <xref rid="pone.0113198-Huang1" ref-type="bibr">[38]</xref>. We compare the performance of our proposed method with four state-of-the-art algorithms, i.e., LMSRC <xref rid="pone.0113198-Wright1" ref-type="bibr">[6]</xref>, RCR <xref rid="pone.0113198-Yang3" ref-type="bibr">[27]</xref>, MTJSRC <xref rid="pone.0113198-Yuan1" ref-type="bibr">[28]</xref> and JDSRC <xref rid="pone.0113198-Zhang1" ref-type="bibr">[29]</xref>. For all face images in each database, we first normalize them in scale and orientation such that eyes are always in the same position, and then crop the facial areas into the final images for recognition. In order to prevent overfitting and fairly compare our algorithm with other algorithms, we randomly split the samples of each database into three disjoint subsets: a training set used to train different recognition algorithms, a validation set for optimizing the parameters in each algorithm and a test set used to assess the recognition performances of various algorithms.</p><p>In local matching-based face recognition methods, a face image can be partitioned into a set of equally or unequally sized sub-images, depending on the user's option. However, how to choose the sub-image size which gives optimal performance is still an open problem. In this work, we will not attempt to deal with this issue. So without loss of generality, equally sized partitions are adopted in our study as in many other approaches <xref rid="pone.0113198-Pentland1" ref-type="bibr">[19]</xref>&#x02013;<xref rid="pone.0113198-Wang3" ref-type="bibr">[25]</xref>.</p><sec id="s3a"><title>Experimental results on the ORL database</title><p>In this subsection, we apply the proposed algorithm to the ORL face database which contains 400 face images of 40 individuals, i.e., 10 images per individual. The images were captured under different lighting conditions, facial expressions (open or closed eyes, smiling or not smiling) and facial details (glasses or without glasses). In our experiments, all images are resized to the resolution of 64&#x000d7;64 pixels with 256 gray levels for computation efficiency. In this database, 4 and 3 images of each person are randomly selected for the training and validation sets, and the remaining samples are regarded as the test set. The random sample selection is repeated 10 times.</p><p>In a first experiment, the impacts of the two parameters (<italic>K</italic> and <inline-formula><inline-graphic xlink:href="pone.0113198.e088.jpg"/></inline-formula>) on the performance of our algorithm under different sub-image sizes are evaluated on the validation set. Here, the sub-image size is set as 32&#x000d7;32, 21&#x000d7;32, 16&#x000d7;32 and 21&#x000d7;16. For the parameters <italic>&#x003bb;</italic> and <italic>K</italic>, we tune their values by searching the grid {0.001, 0.01, 0.05, 0.1, 1, 10, 100, 1000}&#x000d7;{5, 10, 15, 20, 25, 30} (where &#x000d7; is the Cartesian product). From the validation results in Tables S1-S4 in <xref ref-type="supplementary-material" rid="pone.0113198.s001">File S1</xref>, the optimal parameter values for which our algorithm gives the best performances for various sub-image sizes can be easily found. Moreover, two other interesting points can also be observed. Firstly, when the <italic>&#x003bb;</italic> value is fixed, a larger <italic>K</italic> will deteriorate the performance of our algorithm. The reason for this phenomenon is that when the sparsity level <italic>K</italic> is large, more training sub-images from the incorrect classes are selected to represent the query sample, thus the recognition rate is reduced. Secondly, we can see that when the sparsity level <italic>K</italic> is fixed, the performance of our algorithm improves as the value of <italic>&#x003bb;</italic> increases when the <italic>&#x003bb;</italic> value is relatively small. However, this trend is not maintained for all <italic>K</italic> values. For the cases of <italic>K</italic>&#x0200a;=&#x0200a;5, 10, 15 and 20, the performance of our algorithm will slightly decrease after it achieves its top recognition rate. Furthermore, we can find that a larger <italic>&#x003bb;</italic> value is more suitable for larger sparsity level. This means that when the number of training samples selected for representation becomes large, it is preferable to magnify the coefficients corresponding to training samples similar to the query sub-image and penalize the dissimilar ones, since similar training samples are more likely belong to the same class of the query sample. At last, we can see that given the standard deviation, the differences among the recognition results of LCJDSRC under a large number of parameter sets are not significant. This indicates the proposed algorithm is not sensitive to the parameters when they are set as appropriate values.</p><p>In the second experiment, the performance of our algorithm is compared with other algorithms on the test set. According to the validation results in Tables S1-S4 in <xref ref-type="supplementary-material" rid="pone.0113198.s001">File S1</xref>, the parameters <italic>K</italic> and <italic>&#x003bb;</italic> in LCJDSRC are set as {<italic>K</italic>&#x0200a;=&#x0200a;10, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;1}, {<italic>K</italic>&#x0200a;=&#x0200a;10, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;1}, {<italic>K</italic>&#x0200a;=&#x0200a;10, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;10} and {<italic>K</italic>&#x0200a;=&#x0200a;10, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;100} for the sub-image size 32&#x000d7;32, 21&#x000d7;32, 16&#x000d7;32 and 21&#x000d7;16, respectively. We also optimized the parameters of the other algorithms in the same manner as for our algorithm. The average recognition results of the algorithms under evaluation over ten independent runs for each experiment can be seen in <xref ref-type="table" rid="pone-0113198-t001">Table 1</xref>. The table shows that LMSRC obtains the worst recognition result among all algorithms. This is because it processes the sub-images of the query face independently. The performances of MTJSRC, JDSRC and RCR are better than LSMRC, since the latent relationships among sub-images are taken into account. For LCJDSRC, we can see that it outperforms all other algorithms, which confirms that both data locality and the joint sparse representation are important to improve face recognition performance. Moreover, the table shows that the recognition performances of all algorithms vary with the sub-image size. For LMSRC, it achieves the best performance under smaller sub-image size. Instead, a larger sub-image size is preferable for MTJSRC, RCR, JDSRC and LCJDSRC. These results show that the proposed algorithm consistently yields better recognition rates than the other algorithms regardless of the sub-image sizes.</p><table-wrap id="pone-0113198-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t001</object-id><label>Table 1</label><caption><title>The average recognition rates (%) and the corresponding standard deviations (%) of different algorithms under various sub-image sizes on the test set of the ORL face database.</title></caption><alternatives><graphic id="pone-0113198-t001-1" xlink:href="pone.0113198.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Size</td><td align="left" rowspan="1" colspan="1">LMSRC</td><td align="left" rowspan="1" colspan="1">MTJSRC</td><td align="left" rowspan="1" colspan="1">RCR</td><td align="left" rowspan="1" colspan="1">JDSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">83.25&#x000b1;1.68</td><td align="left" rowspan="1" colspan="1">85.41&#x000b1;2.05</td><td align="left" rowspan="1" colspan="1">87.91&#x000b1;2.12</td><td align="left" rowspan="1" colspan="1">88.42&#x000b1;1.78</td><td align="left" rowspan="1" colspan="1">91.92&#x000b1;1.80</td></tr><tr><td align="left" rowspan="1" colspan="1">21&#x000d7;32</td><td align="left" rowspan="1" colspan="1">82.00&#x000b1;1.97</td><td align="left" rowspan="1" colspan="1">85.00&#x000b1;2.69</td><td align="left" rowspan="1" colspan="1">86.58&#x000b1;2.37</td><td align="left" rowspan="1" colspan="1">86.75&#x000b1;3.21</td><td align="left" rowspan="1" colspan="1">90.50&#x000b1;2.16</td></tr><tr><td align="left" rowspan="1" colspan="1">16&#x000d7;32</td><td align="left" rowspan="1" colspan="1">83.41&#x000b1;2.23</td><td align="left" rowspan="1" colspan="1">84.66&#x000b1;2.39</td><td align="left" rowspan="1" colspan="1">86.75&#x000b1;2.30</td><td align="left" rowspan="1" colspan="1">87.00&#x000b1;2.42</td><td align="left" rowspan="1" colspan="1">90.67&#x000b1;2.38</td></tr><tr><td align="left" rowspan="1" colspan="1">16&#x000d7;21</td><td align="left" rowspan="1" colspan="1">83.08&#x000b1;2.48</td><td align="left" rowspan="1" colspan="1">84.75&#x000b1;2.54</td><td align="left" rowspan="1" colspan="1">86.41&#x000b1;1.88</td><td align="left" rowspan="1" colspan="1">86.66&#x000b1;1.92</td><td align="left" rowspan="1" colspan="1">89.00&#x000b1;1.66</td></tr></tbody></table></alternatives></table-wrap><p>Finally, in order to further demonstrate the superiority of our algorithm to the other algorithms, the one-tailed Wilcoxon rank sum test is utilized in this study to verify whether LCJDSRC performs significantly better than the other algorithms. In this test, the null hypothesis is that LCJDSRC makes no difference when compared to the other local matching-based algorithms, and the alternative hypothesis is that LCJDSRC makes an improvement when compared to the other algorithms. For example, if we want to compare the performance of our algorithm with that of LMSRC (LCJDSRC vs. LMSRC), the null and alternative hypotheses can be defined as H<sub>0</sub>: <inline-formula><inline-graphic xlink:href="pone.0113198.e089.jpg"/></inline-formula>
<inline-formula><inline-graphic xlink:href="pone.0113198.e090.jpg"/></inline-formula> and H<sub>1</sub>:<inline-formula><inline-graphic xlink:href="pone.0113198.e091.jpg"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="pone.0113198.e092.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0113198.e093.jpg"/></inline-formula> are the medians of the recognition rates obtained by LCJDSRC and LMSRC. In our experiments, the significance level is set to 1%. From the test results in <xref ref-type="table" rid="pone-0113198-t002">Table 2</xref>, we can find that the <italic>p</italic>-values obtained by all pairwise Wilcoxon rank sum tests are much less than 0.01, which means the null hypotheses are rejected in all pairwise tests and the proposed algorithm significantly outperforms the other algorithms.</p><table-wrap id="pone-0113198-t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t002</object-id><label>Table 2</label><caption><title>The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests on the test set of the ORL database.</title></caption><alternatives><graphic id="pone-0113198-t002-2" xlink:href="pone.0113198.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">21&#x000d7;32</td><td align="left" rowspan="1" colspan="1">16&#x000d7;32</td><td align="left" rowspan="1" colspan="1">16&#x000d7;21</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">LCJDSRC vs. LMSRC</td><td align="left" rowspan="1" colspan="1">8.34e-05</td><td align="left" rowspan="1" colspan="1">8.73e-05</td><td align="left" rowspan="1" colspan="1">1.78e-04</td><td align="left" rowspan="1" colspan="1">1.75e-04</td></tr><tr><td align="left" rowspan="1" colspan="1">LCJDSRC vs. MTJSRC</td><td align="left" rowspan="1" colspan="1">9.65e-05</td><td align="left" rowspan="1" colspan="1">4.80e-04</td><td align="left" rowspan="1" colspan="1">3.04e-04</td><td align="left" rowspan="1" colspan="1">3.96e-04</td></tr><tr><td align="left" rowspan="1" colspan="1">LCJDSRC vs. RCR</td><td align="left" rowspan="1" colspan="1">3.94e-04</td><td align="left" rowspan="1" colspan="1">1.03e-03</td><td align="left" rowspan="1" colspan="1">1.68e-03</td><td align="left" rowspan="1" colspan="1">2.45e-03</td></tr><tr><td align="left" rowspan="1" colspan="1">LCJDSRC vs. JDSRC</td><td align="left" rowspan="1" colspan="1">9.90e-04</td><td align="left" rowspan="1" colspan="1">1.32e-03</td><td align="left" rowspan="1" colspan="1">2.18e-03</td><td align="left" rowspan="1" colspan="1">8.02e-03</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="s3b"><title>Experimental results on the Extended YaleB database</title><p>In this subsection, the performance of the proposed algorithm is evaluated using the Extended YaleB face database, which contains 2414 frontal views of face images from 38 individuals. For each individual, about 64 images were taken under various laboratory-controlled lighting conditions. In our experiment, all images are cropped and resized to the resolution of 64&#x000d7;64 pixels. We randomly select 10 images as training set, 20 images as validation set and the remaining images as test set for each person. This random selection operation is repeated 10 times.</p><p>The performances of the proposed algorithm under different parameters values on the validation set are tested firstly. In this experiment, the sub-image size is set as 32&#x000d7;32 and 21&#x000d7;32. Since the number of training samples in this database is much larger than ORL, we tune the values of <italic>K</italic> and <italic>&#x003bb;</italic> by searching the grid {30, 40, 50, 60, 70, 80, 90}&#x000d7;{0.001, 0.01, 0.05, 0.1, 1, 10, 100, 1000} (where &#x000d7; is the Cartesian product). Tables S5 and S6 in <xref ref-type="supplementary-material" rid="pone.0113198.s001">File S1</xref> show the average recognition results of our algorithm under different parameter values. From these tables, it can be found that with the increase of sparsity level, the recognition performance of the proposed algorithm is generally improved. What's more, we can also observe that when the value of sparsity level is small, LCJDSRC performs better under smaller <italic>&#x003bb;</italic> values, while a relatively larger <italic>&#x003bb;</italic> is preferred at large sparsity levels. Finally, LCJDSRC achieves its best recognition results as 98.01% and 98.20% when the parameters are set to {<italic>K</italic>&#x0200a;=&#x0200a;80, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;100} and {<italic>K</italic>&#x0200a;=&#x0200a;80, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;10} for the sub-image size 32&#x000d7;32 and 21&#x000d7;32, respectively.</p><p>Secondly, we assess the performance of LCJDSRC and compare it with LMSRC, MTJSRC, RCR and JDSRC on the test set. The parameter values for all algorithms are set according to their optimization results on the validation set. In our algorithm, the sparsity level and tradeoff parameter are set as {<italic>K</italic>&#x0200a;=&#x0200a;80, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;100} and {<italic>K</italic>&#x0200a;=&#x0200a;80, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;10}. From the average recognition rates and standard deviations of different algorithms obtained by ten independent repetitions of the experiment reported in <xref ref-type="table" rid="pone-0113198-t003">Table 3</xref>, it can be seen that LCJDSRC outperforms the other algorithms. Furthermore, we can find that the performances of multi-task learning-based algorithms (MTJSRC, RCR, JDSRC and LCJDSRC) are all better than LMSRC. These two observations are consistent with the results obtained on the ORL database. Besides, the one-tailed Wilcoxon rank sum test is also utilized to verify whether the performance of the proposed algorithm is significantly better than the existing algorithms. The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests are listed in <xref ref-type="table" rid="pone-0113198-t004">Table 4</xref>. From these results, we can see that our algorithm significantly outperforms the other algorithms.</p><table-wrap id="pone-0113198-t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t003</object-id><label>Table 3</label><caption><title>The average recognition rates (%) and the corresponding standard deviations (%) of different algorithms under various sub-image sizes on the test set of the Extended YaleB face database.</title></caption><alternatives><graphic id="pone-0113198-t003-3" xlink:href="pone.0113198.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Size</td><td align="left" rowspan="1" colspan="1">LMSRC</td><td align="left" rowspan="1" colspan="1">MTJSRC</td><td align="left" rowspan="1" colspan="1">RCR</td><td align="left" rowspan="1" colspan="1">JDSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">88.51&#x000b1;1.68</td><td align="left" rowspan="1" colspan="1">91.13&#x000b1;1.10</td><td align="left" rowspan="1" colspan="1">92.89&#x000b1;1.40</td><td align="left" rowspan="1" colspan="1">93.35&#x000b1;1.02</td><td align="left" rowspan="1" colspan="1">96.67&#x000b1;1.04</td></tr><tr><td align="left" rowspan="1" colspan="1">21&#x000d7;32</td><td align="left" rowspan="1" colspan="1">87.30&#x000b1;1.51</td><td align="left" rowspan="1" colspan="1">90.73&#x000b1;1.20</td><td align="left" rowspan="1" colspan="1">92.55&#x000b1;1.08</td><td align="left" rowspan="1" colspan="1">92.41&#x000b1;1.29</td><td align="left" rowspan="1" colspan="1">95.48&#x000b1;0.97</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone-0113198-t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t004</object-id><label>Table 4</label><caption><title>The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests on the test set of the Extended YaleB database.</title></caption><alternatives><graphic id="pone-0113198-t004-4" xlink:href="pone.0113198.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Size</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. LMSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. MTJSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. RCR</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. JDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">9.08e-05</td><td align="left" rowspan="1" colspan="1">9.08e-05</td><td align="left" rowspan="1" colspan="1">1.64e-04</td><td align="left" rowspan="1" colspan="1">8.98e-05</td></tr><tr><td align="left" rowspan="1" colspan="1">21&#x000d7;32</td><td align="left" rowspan="1" colspan="1">9.13e-05</td><td align="left" rowspan="1" colspan="1">9.08e-05</td><td align="left" rowspan="1" colspan="1">1.22e-04</td><td align="left" rowspan="1" colspan="1">1.23e-04</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="s3c"><title>Experimental results on the AR database</title><p>In this section, we evaluate the performance of our algorithm using the AR database. This database consists of more than 4000 frontal images from 126 subjects including 70 men and 56 women. For each subject, 26 images were taken under different conditions, including illumination, expression, and facial occlusion/disguise. In our experiments, we select a subset which contains 50 males and 50 females from this database. All images are cropped and resized to the resolution of 64&#x000d7;64 pixels.</p><p>In the first experiment, 14 images of each individual with only illumination and expression changes are selected. Among these images, 6 images from each person are randomly chosen for training, 4 images are used for validation and the remaining images are utilized for testing. This random selection operation is also repeated 10 times. Similar to Section 3.2, we first set the sub-image size as 32&#x000d7;32 and 21&#x000d7;32, and then find the optimal parameter values of the proposed algorithm using the validation set. From the results in Tables S7 and S8 in <xref ref-type="supplementary-material" rid="pone.0113198.s001">File S1</xref>, it can be found that the optimal values of <italic>K</italic> and &#x003bb; for sub-image size of 32&#x000d7;32 and 21&#x000d7;32 are {<italic>K</italic>&#x0200a;=&#x0200a;35, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;0.1} and {<italic>K</italic>&#x0200a;=&#x0200a;30, <italic>&#x003bb;</italic>&#x0200a;=&#x0200a;0.1}, respectively. Furthermore, we can also see that the influence of parameter values on the performance of LCJDSRC is consistent with the observations in Section 3.1 and 3.2. Next, the recognition results of our algorithm are compared to the other approaches on the test set. Here, the optimal parameter values in LMSRC, MTJSRC, RCR and JDSRC are obtained in the same way as for LCJDSRC. The average recognition rates obtained by 10 independent runs for each experiment in <xref ref-type="table" rid="pone-0113198-t005">Table 5</xref> shows that our algorithm outperforms the other algorithms, which is consistent with the experimental results in Section 3.1 and 3.2. However, we can see that our algorithm performs better under the smaller sub-image size, which is opposite to the experimental results on the ORL and Extended YaleB databases. This may happen because the expression variance in this database is much larger than those in the ORL and Extended YaleB, thus a smaller sub-image size is preferable to capture the local facial features which do not vary with the facial expressions. Finally, the <italic>p</italic>-values of pairwise one-tailed Wilcoxon rank sum tests in <xref ref-type="table" rid="pone-0113198-t006">Table 6</xref> show that the recognition performance of the proposed algorithm is significantly better than the other algorithms.</p><table-wrap id="pone-0113198-t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t005</object-id><label>Table 5</label><caption><title>The average recognition rates (%) and the corresponding standard deviations (%) of different algorithms under various sub-image sizes on the test set of the AR face database.</title></caption><alternatives><graphic id="pone-0113198-t005-5" xlink:href="pone.0113198.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LMSRC</td><td align="left" rowspan="1" colspan="1">MTJSRC</td><td align="left" rowspan="1" colspan="1">RCR</td><td align="left" rowspan="1" colspan="1">JDSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">90.62&#x000b1;1.35</td><td align="left" rowspan="1" colspan="1">92.90&#x000b1;1.75</td><td align="left" rowspan="1" colspan="1">95.07&#x000b1;1.21</td><td align="left" rowspan="1" colspan="1">94.42&#x000b1;0.63</td><td align="left" rowspan="1" colspan="1">97.68&#x000b1;0.51</td></tr><tr><td align="left" rowspan="1" colspan="1">21&#x000d7;32</td><td align="left" rowspan="1" colspan="1">91.82&#x000b1;1.33</td><td align="left" rowspan="1" colspan="1">94.05&#x000b1;1.56</td><td align="left" rowspan="1" colspan="1">95.80&#x000b1;1.08</td><td align="left" rowspan="1" colspan="1">94.95&#x000b1;0.87</td><td align="left" rowspan="1" colspan="1">97.90&#x000b1;0.70</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone-0113198-t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t006</object-id><label>Table 6</label><caption><title>The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests on the test set of the AR face database.</title></caption><alternatives><graphic id="pone-0113198-t006-6" xlink:href="pone.0113198.t006"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LCJDSRC vs. LMSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. MTJSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. RCR</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. JDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">8.93e-05</td><td align="left" rowspan="1" colspan="1">8.78e-05</td><td align="left" rowspan="1" colspan="1">8.88e-05</td><td align="left" rowspan="1" colspan="1">8.83e-05</td></tr><tr><td align="left" rowspan="1" colspan="1">21&#x000d7;32</td><td align="left" rowspan="1" colspan="1">8.58e-05</td><td align="left" rowspan="1" colspan="1">9.82e-05</td><td align="left" rowspan="1" colspan="1">1.54e-04</td><td align="left" rowspan="1" colspan="1">8.34e-05</td></tr></tbody></table></alternatives></table-wrap><p>The second experiment on the AR database is run to test the effectiveness of our algorithm under severe occlusion conditions. In this experiment, 1400 images with illumination and expression variations from the database are selected for training, 600 images with sunglasses and scarf occlusions are selected for validation and the other 600 images with sunglasses and scarf occlusions are utilized for testing. We optimize the parameter values of different algorithms using the validation set and then compare their recognition performances on the test set. From the validation results in Tables S9-S12 in the <xref ref-type="supplementary-material" rid="pone.0113198.s001">File S1</xref>, it is easy to find the best parameter values for the proposed algorithm. From the comparison results of various algorithms obtained by ten independent repetitions of the experiment on the test set in <xref ref-type="table" rid="pone-0113198-t007">Tables 7</xref> and <xref ref-type="table" rid="pone-0113198-t008">8</xref>, the following points can be observed. Firstly, it can be found that when the face images are occluded by the sunglasses, the recognition performances obtained by all algorithms are relatively low. However, if the faces are occluded by a scarf, the performances of several algorithms improve. This happens because the sunglasses occlude the eyebrows and eyes in the face, which are proved to be the most important components for face recognition <xref rid="pone.0113198-Sinha1" ref-type="bibr">[26]</xref>. Secondly, we can see that a smaller sub-image size (21&#x000d7;32) is more suitable for the local matching-based algorithms to deal with the face image with occlusions. Finally, it can be obviously seen that our algorithm outperforms the other algorithms. Furthermore, the superiority of our algorithm can also be proved by the pairwise one-tailed Wilcoxon rank sum test results in <xref ref-type="table" rid="pone-0113198-t009">Tables 9</xref>&#x02013;<xref ref-type="table" rid="pone-0113198-t010">10</xref>.</p><table-wrap id="pone-0113198-t007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t007</object-id><label>Table 7</label><caption><title>The average recognition rates (%) and the corresponding standard deviations (%) of different algorithms on the test set of the AR face database with sunglasses and scarf occlusions (sub-image size 32&#x000d7;32).</title></caption><alternatives><graphic id="pone-0113198-t007-7" xlink:href="pone.0113198.t007"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LMSRC</td><td align="left" rowspan="1" colspan="1">MTJSRC</td><td align="left" rowspan="1" colspan="1">RCR</td><td align="left" rowspan="1" colspan="1">JDSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Sunglasses</td><td align="left" rowspan="1" colspan="1">64.30&#x000b1;1.65</td><td align="left" rowspan="1" colspan="1">70.86&#x000b1;1.87</td><td align="left" rowspan="1" colspan="1">73.06&#x000b1;1.67</td><td align="left" rowspan="1" colspan="1">79.13&#x000b1;1.54</td><td align="left" rowspan="1" colspan="1">81.30&#x000b1;1.13</td></tr><tr><td align="left" rowspan="1" colspan="1">Scarf</td><td align="left" rowspan="1" colspan="1">83.13&#x000b1;0.89</td><td align="left" rowspan="1" colspan="1">85.50&#x000b1;1.05</td><td align="left" rowspan="1" colspan="1">88.73&#x000b1;1.09</td><td align="left" rowspan="1" colspan="1">87.76&#x000b1;0.78</td><td align="left" rowspan="1" colspan="1">90.86&#x000b1;0.84</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone-0113198-t008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t008</object-id><label>Table 8</label><caption><title>The average recognition rates (%) and the corresponding standard deviations (%) of different algorithms on the test set of the AR face database with sunglasses and scarf occlusions (sub-image size 21&#x000d7;32).</title></caption><alternatives><graphic id="pone-0113198-t008-8" xlink:href="pone.0113198.t008"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LMSRC</td><td align="left" rowspan="1" colspan="1">MTJSRC</td><td align="left" rowspan="1" colspan="1">RCR</td><td align="left" rowspan="1" colspan="1">JDSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Sunglasses</td><td align="left" rowspan="1" colspan="1">83.30&#x000b1;0.89</td><td align="left" rowspan="1" colspan="1">86.00&#x000b1;1.26</td><td align="left" rowspan="1" colspan="1">89.20&#x000b1;0.86</td><td align="left" rowspan="1" colspan="1">90.20&#x000b1;0.75</td><td align="left" rowspan="1" colspan="1">92.76&#x000b1;0.72</td></tr><tr><td align="left" rowspan="1" colspan="1">Scarf</td><td align="left" rowspan="1" colspan="1">87.73&#x000b1;1.06</td><td align="left" rowspan="1" colspan="1">88.83&#x000b1;1.34</td><td align="left" rowspan="1" colspan="1">91.40&#x000b1;1.26</td><td align="left" rowspan="1" colspan="1">91.46&#x000b1;1.11</td><td align="left" rowspan="1" colspan="1">94.83&#x000b1;0.61</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone-0113198-t009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t009</object-id><label>Table 9</label><caption><title>The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests on the test set of the AR face database with sunglasses and scarf occlusions (sub-image size 32&#x000d7;32).</title></caption><alternatives><graphic id="pone-0113198-t009-9" xlink:href="pone.0113198.t009"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LCJDSRC vs. LMSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. MTJSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. RCR</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. JDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Sunglasses</td><td align="left" rowspan="1" colspan="1">8.83e-05</td><td align="left" rowspan="1" colspan="1">8.83e-05</td><td align="left" rowspan="1" colspan="1">8.93e-05</td><td align="left" rowspan="1" colspan="1">3.58 e-03</td></tr><tr><td align="left" rowspan="1" colspan="1">Scarf</td><td align="left" rowspan="1" colspan="1">8.34e-05</td><td align="left" rowspan="1" colspan="1">8.58e-05</td><td align="left" rowspan="1" colspan="1">6.32e-05</td><td align="left" rowspan="1" colspan="1">8.58e-05</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone-0113198-t010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t010</object-id><label>Table 10</label><caption><title>The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests on the test set of the AR face database with sunglasses and scarf occlusions (sub-image size 21&#x000d7;32).</title></caption><alternatives><graphic id="pone-0113198-t010-10" xlink:href="pone.0113198.t010"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LCJDSRC vs. LMSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. MTJSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. RCR</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. JDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Sunglasses</td><td align="left" rowspan="1" colspan="1">7.78e-05</td><td align="left" rowspan="1" colspan="1">7.92e-05</td><td align="left" rowspan="1" colspan="1">7.92e-05</td><td align="left" rowspan="1" colspan="1">7.92e-05</td></tr><tr><td align="left" rowspan="1" colspan="1">Scarf</td><td align="left" rowspan="1" colspan="1">8.25e-05</td><td align="left" rowspan="1" colspan="1">8.20e-05</td><td align="left" rowspan="1" colspan="1">9.65e-05</td><td align="left" rowspan="1" colspan="1">8.39e-05</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="s3d"><title>Experimental results on the LFW database</title><p>The LFW database <xref rid="pone.0113198-Huang1" ref-type="bibr">[38]</xref> is a large scale database which contains 13,233 target face images of 5,749 different individuals. Since all the samples were taken from the real world in an unconstrained environment, facial expressions, pose, illumination, occlusions and alignment are very variable in this database. As suggested in <xref rid="pone.0113198-Zhu1" ref-type="bibr">[39]</xref>, a subset which contains 1580 face images of 158 individuals from the LFW-a <xref rid="pone.0113198-Wolf1" ref-type="bibr">[40]</xref> database is employed in our study. In this subset, each individual has 10 images with the size of 32&#x000d7;32 pixel (The mat file can be download from <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/~cslzhang/code/MPCRC_eccv12_code.zip">http://www4.comp.polyu.edu.hk/~cslzhang/code/MPCRC_eccv12_code.zip</ext-link>). In our experiment, 6 samples of each individual are randomly selected for training. Among the remaining 4 samples, 2 images of each individual are randomly chosen for validation and the other 2 images are used for testing. This random selection operation is also repeated 10 times. Here, we equally partition the face images into four sub-images and the sub-image size is 16&#x000d7;16.</p><p>Firstly, the performances of the proposed algorithm under various parameter values are tested on the validation set to find the optimal parameters for our algorithm. From the validation results in Table S13 in the <xref ref-type="supplementary-material" rid="pone.0113198.s001">File S1</xref>, we can see that the influence of the two parameters on the proposed algorithm is similar to those in Section 3.1-3.3, and the optimal parameter values for which our algorithm gives the best recognition rate are <italic>K</italic>&#x0200a;=&#x0200a;50 and <italic>&#x003bb;</italic> &#x0200a;=&#x0200a;0.05.</p><p>Then, we compare the proposed algorithm with other algorithms on the test set. From the average recognition rate of each algorithm over ten independent runs for each experiment in <xref ref-type="table" rid="pone-0113198-t011">Table 11</xref> and the Wilcoxon rank sum test results in <xref ref-type="table" rid="pone-0113198-t012">Table 12</xref>, it can be said that, although the recognition performances of all algorithms on the LFW database are relatively lower than those on the other three databases, our algorithm is still significantly superior to the other algorithms.</p><table-wrap id="pone-0113198-t011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t011</object-id><label>Table 11</label><caption><title>The average recognition rates (%) and the corresponding standard deviations (%) of different algorithms on the test set of the LFW face database (sub-image size 32&#x000d7;32).</title></caption><alternatives><graphic id="pone-0113198-t011-11" xlink:href="pone.0113198.t011"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LMSRC</td><td align="left" rowspan="1" colspan="1">MTJSRC</td><td align="left" rowspan="1" colspan="1">RCR</td><td align="left" rowspan="1" colspan="1">JDSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">35.88&#x000b1;1.36</td><td align="left" rowspan="1" colspan="1">42.97&#x000b1;2.48</td><td align="left" rowspan="1" colspan="1">44.71&#x000b1;2.23</td><td align="left" rowspan="1" colspan="1">45.53&#x000b1;2.20</td><td align="left" rowspan="1" colspan="1">49.34&#x000b1;0.99</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone-0113198-t012" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0113198.t012</object-id><label>Table 12</label><caption><title>The <italic>p</italic>-values of the pairwise one-tailed Wilcoxon rank sum tests on the test set of the LFW face database (sub-image size 32&#x000d7;32).</title></caption><alternatives><graphic id="pone-0113198-t012-12" xlink:href="pone.0113198.t012"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LCJDSRC vs. LMSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. MTJSRC</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. RCR</td><td align="left" rowspan="1" colspan="1">LCJDSRC vs. JDSRC</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">32&#x000d7;32</td><td align="left" rowspan="1" colspan="1">8.93e-05</td><td align="left" rowspan="1" colspan="1">8.98e-05</td><td align="left" rowspan="1" colspan="1">8.93e-05</td><td align="left" rowspan="1" colspan="1">1.04e-04</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec id="s4"><title>Conclusion and Future Work</title><p>In this paper, a novel classification algorithm named Locality Constrained Joint Dynamic Sparse Representation-based Classification (LCJDSRC) has been proposed for local matching-based face recognition. Our algorithm combines the joint sparse representation and locality constraint into a unified framework. Therefore, not only does it consider the latent relationships among different sub-images of a face, but also introduces the locality information into the sparse representation model. Moreover, a greedy algorithm based on Matching Pursuit (MP) has been presented to optimize the objective function of LCJDSRC. Extensive experiments have been carried out on four databases including ORL, Extended YaleB, AR and LFW to demonstrate the effectiveness of our proposed LCJDSRC approach. The experimental results have shown that LCJDSRC outperforms several similar methods such as LMSRC, MTJSRC, RCR and JDSRC on the data sets considered in our tests.</p><p>Finally, it should be pointed out that in LCJDSRC, the query sub-images are represented by the sub-images partitioned from the original training samples, which may decrease its performance when too few training samples are available. Thus, one of our future goals is to incorporate our algorithm into the dictionary learning framework <xref rid="pone.0113198-Tosic1" ref-type="bibr">[41]</xref>, <xref rid="pone.0113198-Ramrez1" ref-type="bibr">[42]</xref> to further improve its flexibility. Besides, since some researchers have shown that the dimensionality reduction methods are helpful to the sparse representation-based classification algorithms, how to combine LCJDSRC with the dimensionality reduction techniques is another interesting topic for future study.</p></sec><sec sec-type="supplementary-material" id="s5"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0113198.s001"><label>File S1</label><caption><p>
<bold>Tables S1-S13 and Text S1.</bold> Table S1. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the ORL face database (sub-image size is 32&#x000d7;32). Table S2. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the ORL face database (sub-image size is 21&#x000d7;32). Table S3. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the ORL face database (sub-image size is 16&#x000d7;32). Table S4. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the ORL face database (sub-image size is 16&#x000d7;21). Table S5. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the Extended YaleB face database (sub-image size is 32&#x000d7;32). Table S6. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the Extended YaleB face database (sub-image size is 21&#x000d7;32). Table S7. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the AR face database (sub-image size is 32&#x000d7;32). Table S8. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the AR face database (sub-image size is 21&#x000d7;32). Table S9. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the AR face database with sunglasses occlusion (sub-image size is 32&#x000d7;32). Table S10. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the AR face database with scarf occlusion (sub-image size is 32&#x000d7;32). Table S11. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the AR face database with sunglasses occlusion (sub-image size is 21&#x000d7;32). Table S12. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the AR face database with scarf occlusion (sub-image size is 21&#x000d7;32). Table S13. The average recognition rates (%) and the corresponding standard deviations (%) of LCJDSRC under different parameters on the validation set of the LFW face database (sub-image size is 32&#x000d7;32). Text S1. The derivation process of <xref ref-type="disp-formula" rid="pone.0113198.e063">Equation (16</xref>).</p><p>(DOC)</p></caption><media xlink:href="pone.0113198.s001.doc"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0113198-Li1"><label>1</label><mixed-citation publication-type="book">Li SZ, Jain AK (2011) Handbook of face recognition, Springer.</mixed-citation></ref><ref id="pone.0113198-Jafri1"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Jafri</surname><given-names>R</given-names></name>, <name><surname>Arabnia</surname><given-names>HR</given-names></name> (<year>2009</year>) <article-title>A survey of face recognition techniques</article-title>. <source>Information Processing Systems</source>
<volume>5(2)</volume>:<fpage>41</fpage>&#x02013;<lpage>68</lpage>.</mixed-citation></ref><ref id="pone.0113198-Zou1"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Zou</surname><given-names>J</given-names></name>, <name><surname>Ji</surname><given-names>Q</given-names></name>, <name><surname>Nagy</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>A comparative study of local matching approach for face recognition</article-title>. <source>IEEE Transactions on Image Processing</source>
<volume>16(10)</volume>:<fpage>2617</fpage>&#x02013;<lpage>2628</lpage>.<pub-id pub-id-type="pmid">17926941</pub-id></mixed-citation></ref><ref id="pone.0113198-Subban1"><label>4</label><mixed-citation publication-type="other">Subban R, Mankame DP (2014) Human face recognition biometric techniques: analysis and review. Recent Advances in Intelligent Informatics: 455&#x02013;463.</mixed-citation></ref><ref id="pone.0113198-Ramirez1"><label>5</label><mixed-citation publication-type="other">Ramirez I, Sapiro G (2010) Universal sparse modeling. Technical Report.</mixed-citation></ref><ref id="pone.0113198-Wright1"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Wright</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>AY</given-names></name>, <name><surname>Ganesh</surname><given-names>A</given-names></name>, <name><surname>Sastry</surname><given-names>SS</given-names></name>, <name><surname>Ma</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Robust face recognition via sparse representation</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
<volume>31(2)</volume>:<fpage>210</fpage>&#x02013;<lpage>227</lpage>.<pub-id pub-id-type="pmid">19110489</pub-id></mixed-citation></ref><ref id="pone.0113198-Gao1"><label>7</label><mixed-citation publication-type="other">Gao S, Tsang IWH, Chia LT (2010) Kernel sparse representation for image classification and face recognition. European Conference on Computer Vision: 1&#x02013;14.</mixed-citation></ref><ref id="pone.0113198-ShaweTaylor1"><label>8</label><mixed-citation publication-type="book">Shawe-Taylor J, Cristianini N (2004) Kernel methods for pattern analysis. Cambridge University Press.</mixed-citation></ref><ref id="pone.0113198-Yang1"><label>9</label><mixed-citation publication-type="other">Yang M, Zhang L (2010) Gabor feature based sparse representation for face recognition with gabor occlusion dictionary. European Conference on Computer Vision: 448&#x02013;461.</mixed-citation></ref><ref id="pone.0113198-Wang1"><label>10</label><mixed-citation publication-type="other">Wang J, Yang J, Yu K, Lv F, Huang T, et al.. (2010) Locality-constrained linear coding for image classification. IEEE Conference on Computer Vision and Pattern Recognition: 3360&#x02013;3367.</mixed-citation></ref><ref id="pone.0113198-Roweis1"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Roweis</surname><given-names>ST</given-names></name>, <name><surname>Saul</surname><given-names>LK</given-names></name> (<year>2000</year>) <article-title>Nonlinear dimensionality reduction by locally linear embedding</article-title>. <source>Science</source>
<volume>290(5500)</volume>:<fpage>2323</fpage>&#x02013;<lpage>2326</lpage>.<pub-id pub-id-type="pmid">11125150</pub-id></mixed-citation></ref><ref id="pone.0113198-Tenenbaum1"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Tenenbaum</surname><given-names>JB</given-names></name>, <name><surname>De Silva</surname><given-names>V</given-names></name>, <name><surname>Langford</surname><given-names>JC</given-names></name> (<year>2000</year>) <article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>. <source>Science</source>
<volume>290(5500)</volume>:<fpage>2319</fpage>&#x02013;<lpage>2323</lpage>.<pub-id pub-id-type="pmid">11125149</pub-id></mixed-citation></ref><ref id="pone.0113198-Elhamifar1"><label>13</label><mixed-citation publication-type="other">Elhamifar E, Vidal R (2011) Robust classification using structured sparse representation. IEEE Conference on Computer Vision and Pattern Recognition: 1873&#x02013;1879.</mixed-citation></ref><ref id="pone.0113198-Wagner1"><label>14</label><mixed-citation publication-type="other">Wagner A, Wright J, Ganesh A, Zhou Z, Mobahi H, et al.. (2009) Towards a practical face recognition system: robust registration and illumination by sparse representation. IEEE Conference on Computer Vision and Pattern Recognition: 597&#x02013;604.</mixed-citation></ref><ref id="pone.0113198-Yang2"><label>15</label><mixed-citation publication-type="other">Yang M, Zhang L, Yang J, Zhang D (2011) Robust sparse coding for face recognition. IEEE Conference on Computer Vision and Pattern Recognition: 625&#x02013;632.</mixed-citation></ref><ref id="pone.0113198-Deng1"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Deng</surname><given-names>W</given-names></name>, <name><surname>Hu</surname><given-names>J</given-names></name>, <name><surname>Guo</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Extended SRC: undersampled face recognition via intraclass variant dictionary</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
<volume>34(9)</volume>:<fpage>1864</fpage>&#x02013;<lpage>1870</lpage>.<pub-id pub-id-type="pmid">22813959</pub-id></mixed-citation></ref><ref id="pone.0113198-Mi1"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Mi</surname><given-names>JX</given-names></name>, <name><surname>Liu</surname><given-names>JX</given-names></name> (<year>2013</year>) <article-title>Face recognition using sparse representation-based classification on <italic>K</italic>-nearest subspace</article-title>. <source>Plos One</source>
<volume>8(3)</volume>:<fpage>e59430</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0059430">10.1371/journal.pone.0059430</ext-link></comment>
<pub-id pub-id-type="pmid">23555671</pub-id></mixed-citation></ref><ref id="pone.0113198-Chen1"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Jin</surname><given-names>Z</given-names></name> (<year>2013</year>) <article-title>Feature extraction based on maximum nearest subspace margin criterion</article-title>. <source>Neural Processing Letters</source>
<volume>37(3)</volume>:<fpage>355</fpage>&#x02013;<lpage>375</lpage>.</mixed-citation></ref><ref id="pone.0113198-Pentland1"><label>19</label><mixed-citation publication-type="other">Pentland A, Moghaddam B, Starner T (1994) View-based and modular eigenspaces for face recognition. IEEE Conference on Computer Vision and Pattern Recognition: 84&#x02013;91.</mixed-citation></ref><ref id="pone.0113198-Chen2"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Chen</surname><given-names>S</given-names></name>, <name><surname>Zhu</surname><given-names>Y</given-names></name> (<year>2004</year>) <article-title>Subpattern-based principle component analysis</article-title>. <source>Pattern Recognition</source>
<volume>37(5)</volume>:<fpage>1081</fpage>&#x02013;<lpage>1083</lpage>.</mixed-citation></ref><ref id="pone.0113198-Nanni1"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Nanni</surname><given-names>L</given-names></name>, <name><surname>Maio</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>Weighted sub-gabor for face recognition</article-title>. <source>Pattern Recognition Letters</source>
<volume>28(4)</volume>:<fpage>487</fpage>&#x02013;<lpage>492</lpage>.</mixed-citation></ref><ref id="pone.0113198-Tan1"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Tan</surname><given-names>K</given-names></name>, <name><surname>Chen</surname><given-names>S</given-names></name> (<year>2005</year>) <article-title>Adaptively weighted sub-pattern PCA for face recognition</article-title>. <source>Neurocomputing</source>
<volume>64</volume>:<fpage>505</fpage>&#x02013;<lpage>511</lpage>.</mixed-citation></ref><ref id="pone.0113198-Kumar1"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Kumar</surname><given-names>KV</given-names></name>, <name><surname>Negi</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>SubXPCA and a generalized feature partitioning approach to principal component analysis</article-title>. <source>Pattern Recognition</source>
<volume>41(4)</volume>:<fpage>1398</fpage>&#x02013;<lpage>1409</lpage>.</mixed-citation></ref><ref id="pone.0113198-Wang2"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>B</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Qi</surname><given-names>M</given-names></name>, <name><surname>Kong</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>An adaptively weighted sub-pattern locality preserving projection for face recognition</article-title>. <source>Journal of Network and Computer Applications</source>
<volume>33(3)</volume>:<fpage>323</fpage>&#x02013;<lpage>332</lpage>.</mixed-citation></ref><ref id="pone.0113198-Wang3"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Ma</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>B</given-names></name>, <name><surname>Qi</surname><given-names>M</given-names></name>, <name><surname>Kong</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>A structure-preserved local matching approach for face recognition</article-title>. <source>Pattern Recognition Letters</source>
<volume>32(3)</volume>:<fpage>494</fpage>&#x02013;<lpage>504</lpage>.</mixed-citation></ref><ref id="pone.0113198-Sinha1"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Sinha</surname><given-names>P</given-names></name>, <name><surname>Balas</surname><given-names>B</given-names></name>, <name><surname>Ostrovsky</surname><given-names>Y</given-names></name>, <name><surname>Russell</surname><given-names>R</given-names></name> (<year>2006</year>) <article-title>Face recognition by humans: nineteen results all computer vision researchers should know about</article-title>. <source>Proceedings of the IEEE</source>
<volume>94(11)</volume>:<fpage>1948</fpage>&#x02013;<lpage>1962</lpage>.</mixed-citation></ref><ref id="pone.0113198-Yang3"><label>27</label><mixed-citation publication-type="other">Yang M, Zhang D, Wang S (2012) Relaxed collaborative representation for pattern classification. IEEE Conference on Computer Vision and Pattern Recognition: 2224&#x02013;2231.</mixed-citation></ref><ref id="pone.0113198-Yuan1"><label>28</label><mixed-citation publication-type="other">Yuan XT, Yan S (2010) Visual classification with multi-task joint sparse representation. IEEE Conference on Computer Vision and Pattern Recognition: 3493&#x02013;3500.</mixed-citation></ref><ref id="pone.0113198-Zhang1"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Nasrabadi</surname><given-names>NM</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Huang</surname><given-names>TS</given-names></name> (<year>2012</year>) <article-title>Joint dynamic sparse representation for multi-view face recognition</article-title>. <source>Pattern Recognition</source>
<volume>45(4)</volume>:<fpage>1290</fpage>&#x02013;<lpage>1298</lpage>.</mixed-citation></ref><ref id="pone.0113198-Chao1"><label>30</label><mixed-citation publication-type="other">Chao YW, Yeh YR, Chen YW, Lee YJ, Wang YCF (2011) Locality-constrained group sparse representation for robust face recognition. IEEE International Conference on Image Processing: 761&#x02013;764.</mixed-citation></ref><ref id="pone.0113198-Wei1"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Wei</surname><given-names>CP</given-names></name>, <name><surname>Chao</surname><given-names>YW</given-names></name>, <name><surname>Yeh</surname><given-names>YR</given-names></name>, <name><surname>Wang</surname><given-names>YCF</given-names></name> (<year>2013</year>) <article-title>Locality-sensitive dictionary learning for sparse representation based classification</article-title>. <source>Pattern Recognition</source>
<volume>46(5)</volume>:<fpage>1277</fpage>&#x02013;<lpage>1287</lpage>.</mixed-citation></ref><ref id="pone.0113198-Yu1"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Yu</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name>, <name><surname>Gong</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Nonlinear learning using local coordinate coding</article-title>. <source>Advances in Neural Information Processing Systems</source>
<volume>22</volume>:<fpage>2223</fpage>&#x02013;<lpage>2231</lpage>.</mixed-citation></ref><ref id="pone.0113198-Mallat1"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Mallat</surname><given-names>SG</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name> (<year>1933</year>) <article-title>Matching pursuits with time-frequency dictionaries</article-title>. <source>IEEE Transactions on Signal Processing</source>
<volume>41(12)</volume>:<fpage>3397</fpage>&#x02013;<lpage>3415</lpage>.</mixed-citation></ref><ref id="pone.0113198-Duarte1"><label>34</label><mixed-citation publication-type="other">Duarte MF, Cevher V, Baraniuk RG (2009) Model-based compressive sensing for signal ensembles. Proceedings of the Allerton Conference on Communication, Control, and Computing: 585&#x02013;600.</mixed-citation></ref><ref id="pone.0113198-Samaria1"><label>35</label><mixed-citation publication-type="other">Samaria FS, Harter AC (1994) Parameterisation of a stochastic model for human face identification. IEEE Workshop on Applications of Computer Vision: 138&#x02013;142.</mixed-citation></ref><ref id="pone.0113198-Georghiades1"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Georghiades</surname><given-names>AS</given-names></name>, <name><surname>Belhumeur</surname><given-names>PN</given-names></name>, <name><surname>Kriegman</surname><given-names>D</given-names></name> (<year>2001</year>) <article-title>From few to many: illumination cone models for face recognition under variable lighting and pose</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
<volume>23(6)</volume>:<fpage>643</fpage>&#x02013;<lpage>660</lpage>.</mixed-citation></ref><ref id="pone.0113198-Martinez1"><label>37</label><mixed-citation publication-type="other">Martinez AM, Benavente R (1998) The AR face database. CVC Technical Report 24..</mixed-citation></ref><ref id="pone.0113198-Huang1"><label>38</label><mixed-citation publication-type="other">Huang GB, Mattar M, Berg T, Learned-Miller E (2007) Labeled face in the wild: a database for studying face recognition in unconstrained environments. Technical Report 07&#x02013;49.</mixed-citation></ref><ref id="pone.0113198-Zhu1"><label>39</label><mixed-citation publication-type="other">Zhu P, Zhang L, Hu Q, Shiu SC (2012) Multi-scale patch based collaborative representation for face recognition with margin distribution optimization. European Conference on Computer Vision: 822&#x02013;835.</mixed-citation></ref><ref id="pone.0113198-Wolf1"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Wolf</surname><given-names>L</given-names></name>, <name><surname>Hassner</surname><given-names>T</given-names></name>, <name><surname>Taigman</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Similarity scores based on background sample</article-title>. <source>Asian Conference on Computer Vision</source>
<fpage>88</fpage>&#x02013;<lpage>89</lpage>.</mixed-citation></ref><ref id="pone.0113198-Tosic1"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Tosic</surname><given-names>I</given-names></name>, <name><surname>Frossard</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Dictionary learning</article-title>. <source>IEEE Signal Processing Magazine</source>
<volume>28(2)</volume>:<fpage>27</fpage>&#x02013;<lpage>38</lpage>.</mixed-citation></ref><ref id="pone.0113198-Ramrez1"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Ram&#x000ed;rez</surname><given-names>I</given-names></name>, <name><surname>Sapiro</surname><given-names>G</given-names></name> (<year>2012</year>) <article-title>An MDL framework for sparse coding and dictionary learning</article-title>. <source>IEEE Transactions on Signal Processing</source>
<volume>60(6)</volume>:<fpage>2913</fpage>&#x02013;<lpage>2927</lpage>.</mixed-citation></ref></ref-list></back></article>
