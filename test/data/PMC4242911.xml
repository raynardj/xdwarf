<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="article-commentary"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Korean J Fam Med</journal-id><journal-id journal-id-type="iso-abbrev">Korean J Fam Med</journal-id><journal-id journal-id-type="publisher-id">KJFM</journal-id><journal-title-group><journal-title>Korean Journal of Family Medicine</journal-title></journal-title-group><issn pub-type="ppub">2005-6443</issn><issn pub-type="epub">2092-6715</issn><publisher><publisher-name>The Korean Academy of Family Medicine</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.4082/kjfm.2014.35.6.325</article-id><article-categories><subj-group subj-group-type="heading"><subject>Commentary</subject></subj-group></article-categories><title-group><article-title>Comments on Statistical Issues in November 2014</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Park</surname><given-names>Yong Gyu</given-names></name><xref ref-type="aff" rid="A1"/></contrib></contrib-group><aff id="A1">Department of Biostatistics, The Catholic University of Korea College of Medicine, Seoul, Korea.</aff><pub-date pub-type="ppub"><month>11</month><year>2014</year></pub-date><pub-date pub-type="epub"><day>21</day><month>11</month><year>2014</year></pub-date><volume>35</volume><issue>6</issue><fpage>325</fpage><lpage>326</lpage><permissions><copyright-statement>Copyright &#x000a9; 2014 The Korean Academy of Family Medicine</copyright-statement><copyright-year>2014</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/">http://creativecommons.org/licenses/by-nc/3.0/</ext-link>) which permits unrestricted noncommercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article related-article-type="commentary-article" id="d35e70" vol="35" page="167" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="24921036" ext-link-type="pubmed"><article-title>Comments on statistical issues in May 2014</article-title></related-article></article-meta></front><body><p>In this section, we continue to explain Cohen's kappa coefficient, which was described in the commentary titled, "Comments on statistical issues in May 2014", by Park<xref rid="B1" ref-type="bibr">1)</xref> published in May 2014, and its related topics.</p><sec><title>KAPPA, DIAGNOSTIC ACCURACY, AND PARADOX OF KAPPA</title><p>Cohen's kappa coefficient is a measure of inter-observer or inter-device agreement for qualitative (categorical) items used in many scientific fields; it accounts for the possibility that the agreement occurred by chance. However, as we mentioned previously,<xref rid="B1" ref-type="bibr">1)</xref> kappa should not be used as a measure of agreement when all raters or devices cannot be treated symmetrically. When one of the sources of ratings may be viewed as superior or a standard (e.g., one rater is senior to the other or one medical device is more precise than the other), kappa may no longer be appropriate.</p><p>The term diagnostic accuracy, which is usually expressed by sensitivity, specificity, and positive and negative predictive values, presupposes that there is an underlying gold-standard (true values). Therefore, these two terms cannot be used interchangeably to explain the same phenomenon. Additionally, 'over-estimate' and 'under-estimate' cannot be used together with 'kappa,' but 'concordance' and 'correlation' can be shared with 'kappa.'</p><p>The interpretation of kappa has been under severe criticism. A well-known problem has been referred to as 'the first paradox of kappa' by Feinstein and Cicchetti.<xref rid="B2" ref-type="bibr">2)</xref> We usually expect the value of kappa to be high when both observers assess one of several categories with a high probability, and low when both observers assess all categories evenly. However, the greater the imbalance in the marginal distributions of each category, the higher the probability of chance agreement; in this case, the magnitude of kappa is reduced considerably, even when the observed agreement is quite high. Such imbalanced marginal distributions often occur when the sample data are obtained from a population with a very low prevalence of the disease under consideration.</p><p>For this reason, 'kappa' is considered an overly conservative measure of agreement. It seems to be more appropriate to use the interclass kappa<xref rid="B3" ref-type="bibr">3)</xref> and the first-order agreement coefficient<xref rid="B4" ref-type="bibr">4)</xref> as measures of agreement for analyses of data with severely imbalanced marginal distributions.</p></sec></body><back><fn-group><fn fn-type="conflict"><p>No potential conflict of interest relevant to this article was reported.</p></fn></fn-group><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>YG</given-names></name></person-group><article-title>Comments on statistical issues in May 2014</article-title><source>Korean J Fam Med</source><year>2014</year><volume>35</volume><fpage>167</fpage><lpage>168</lpage><pub-id pub-id-type="pmid">24921036</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinstein</surname><given-names>AR</given-names></name><name><surname>Cicchetti</surname><given-names>DV</given-names></name></person-group><article-title>High agreement but low kappa: I. the problems of two paradoxes</article-title><source>J Clin Epidemiol</source><year>1990</year><volume>43</volume><fpage>543</fpage><lpage>549</lpage><pub-id pub-id-type="pmid">2348207</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloch</surname><given-names>DA</given-names></name><name><surname>Kraemer</surname><given-names>HC</given-names></name></person-group><article-title>2 &#x000d7; 2 kappa coefficients: measures of agreement or association</article-title><source>Biometrics</source><year>1989</year><volume>45</volume><fpage>269</fpage><lpage>287</lpage><pub-id pub-id-type="pmid">2655731</pub-id></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gwet</surname><given-names>KL</given-names></name></person-group><source>Handbook of inter-rater reliability</source><publisher-loc>Gaithersburg</publisher-loc><publisher-name>STATAXIS Publishing Company</publisher-name><year>2001</year></element-citation></ref></ref-list></back></article>
